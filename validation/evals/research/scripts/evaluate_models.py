#!/usr/bin/env python
import asyncio
import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any

import pandas as pd
from tqdm import tqdm

PROJECT_ROOT = Path(__file__).resolve().parents[3]
EVALS_ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(EVALS_ROOT))

from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

from core.config import config
from core.llm import get_llm
from core.llm.chains import get_retrieval_chain
from core.ranking import get_doc_content, rerank_documents
from core.vector_store import get_vector_store
from research import research_config
from research.evals.evaluation import (
    cleanup_models,
    evaluate_batch,
    evaluate_dataset,
    generate_report,
    save_results,
)

os.makedirs("research/logs", exist_ok=True)

MODELS_TO_EVALUATE = research_config.get(
    "models_to_evaluate", ["google/gemini-2.5-flash-preview-05-20"]
)
TEMPERATURE = research_config.get("evaluation", {}).get("generation_temperature", 0.0)
EVAL_MODEL_NAME = research_config.get("evaluation", {}).get(
    "eval_model", "google/gemini-2.5-flash-preview-05-20"
)
TEST_DATASET_PATH = research_config.get("data", {}).get(
    "test_dataset", "research/data/test_dataset.csv"
)
LIMIT = research_config.get("evaluation", {}).get("default_limit", 20)
MAX_CONCURRENCY = research_config.get("evaluation", {}).get("max_concurrency", 3)

# –ö—ç—à –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –≤—Ä–µ–º–µ–Ω–∏
_document_cache: dict[str, Any] = {}


def precompute_documents_for_all_questions(
    dataset: pd.DataFrame, limit: int | None = None
) -> pd.DataFrame:
    """–ü—Ä–µ–¥–ø–æ—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –≤—Å–µ—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –æ–¥–∏–Ω —Ä–∞–∑"""
    if limit is not None and limit < len(dataset):
        dataset = dataset.sample(limit, random_state=42).reset_index(drop=True)

    global _document_cache
    _document_cache.clear()

    print(f"üîç –ü—Ä–µ–¥–ø–æ—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è {len(dataset)} –≤–æ–ø—Ä–æ—Å–æ–≤...")

    vector_store = get_vector_store()
    use_reranker = config.get("reranker", {}).get("enabled", False)
    search_top_k = config.get("database", {}).get("search_top_k", 10)
    rerank_top_k = config.get("reranker", {}).get("top_k", 5)

    for idx, (_, row) in enumerate(
        tqdm(dataset.iterrows(), total=len(dataset), desc="–ü–æ–ª—É—á–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    ):
        question = row["question"]

        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
        docs = vector_store.similarity_search(question, k=search_top_k)
        print(f"–í–æ–ø—Ä–æ—Å {idx + 1}: –ù–∞–π–¥–µ–Ω–æ {len(docs)} —á–∞–Ω–∫–æ–≤")

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–µ—Ä–∞–Ω–∫–µ—Ä –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω
        if use_reranker and docs:
            docs = rerank_documents(question, docs)[:rerank_top_k]
            print(f"–í–æ–ø—Ä–æ—Å {idx + 1}: –ü–æ—Å–ª–µ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        docs_separator = config.get("docs_separator", "\n\n-----")
        formatted_context = docs_separator.join(get_doc_content(doc) for doc in docs)

        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        _document_cache[question] = formatted_context

    print(f"‚úÖ –ü—Ä–µ–¥–ø–æ—Å—á—ë—Ç –∑–∞–≤–µ—Ä—à—ë–Ω! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(_document_cache)} –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤")
    return dataset


def create_model_specific_chain(model_name: str):
    """–°–æ–∑–¥–∞–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é —Ü–µ–ø–æ—á–∫—É –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–ø–æ—Å—á–∏—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    # –ë–µ—Ä–µ–º –±–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ–º –ø–æ–¥ –º–æ–¥–µ–ª—å
    qa_prompt = config.get("qa_prompt", "–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n\n–í–æ–ø—Ä–æ—Å: {question}\n\n–û—Ç–≤–µ—Ç:")

    # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è Qwen –º–æ–¥–µ–ª–µ–π
    if "qwen" in model_name.lower():
        qa_prompt = "/no_think\n\n" + qa_prompt

    prompt = ChatPromptTemplate.from_template(qa_prompt)

    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä LLM –¥–ª—è —ç—Ç–æ–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
    llm = get_llm(temperature=TEMPERATURE)
    # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –≤ —Ç–µ–∫—É—â–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ get_llm –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç model_name
    # –í–æ–∑–º–æ–∂–Ω–æ, –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

    def get_cached_context(query):
        """–ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–µ–¥–ø–æ—Å—á–∏—Ç–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∫—ç—à–∞"""
        global _document_cache
        return _document_cache.get(query, "–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

    rag_chain = (
        {"context": get_cached_context, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return rag_chain


async def generate_system_responses_async(
    dataset: pd.DataFrame,
    model_name: str,
    limit: int | None = None,
    max_concurrent_questions: int = 6,
) -> pd.DataFrame:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    if limit is not None and limit < len(dataset):
        dataset = dataset.sample(limit, random_state=42).reset_index(drop=True)

    # –°–æ–∑–¥–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é —Ü–µ–ø–æ—á–∫—É –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏
    retrieval_chain = create_model_specific_chain(model_name)

    # –°–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏
    semaphore = asyncio.Semaphore(max_concurrent_questions)

    async def process_question(question: str, golden_answer: str, question_idx: int):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –≤–æ–ø—Ä–æ—Å"""
        async with semaphore:
            print(f"\n[{model_name}] –í–æ–ø—Ä–æ—Å {question_idx + 1}: {question}")

            try:
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                result = await retrieval_chain.ainvoke(question)

                return {
                    "question": question,
                    "system_answer": result,
                    "golden_answer": golden_answer,
                    "model": model_name,
                }
            except Exception as e:
                print(f"‚ö†Ô∏è [{model_name}] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞ {question_idx + 1}: {e}")
                return {
                    "question": question,
                    "system_answer": f"–û—à–∏–±–∫–∞: {str(e)}",
                    "golden_answer": golden_answer,
                    "model": model_name,
                }

    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
    tasks = [
        process_question(row["question"], row["answer"], idx)
        for idx, (_, row) in enumerate(dataset.iterrows())
    ]

    # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏
    results = await asyncio.gather(*tasks)

    return pd.DataFrame(results)


async def evaluate_model(
    model_name: str, dataset: pd.DataFrame, output_dir: str, limit: int | None = None
):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ–¥–Ω—É –º–æ–¥–µ–ª—å"""
    print(f"\nüî¨ –ù–∞—á–∏–Ω–∞–µ–º –æ—Ü–µ–Ω–∫—É –º–æ–¥–µ–ª–∏: {model_name}")
    start_time = time.time()

    try:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç—ã —Å–∏—Å—Ç–µ–º—ã
        system_responses = await generate_system_responses_async(
            dataset,
            model_name,
            limit=limit,
            max_concurrent_questions=3,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        )

        print(f"‚úÖ [{model_name}] –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(system_responses)} –æ—Ç–≤–µ—Ç–æ–≤")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–≤–µ—Ç—ã —Å–∏—Å—Ç–µ–º—ã
        responses_path = os.path.join(output_dir, f"{model_name.replace('/', '_')}_responses.csv")
        system_responses.to_csv(responses_path, index=False)

        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
        actual_contexts = [
            _document_cache.get(row["question"], "") for _, row in dataset.iterrows()
        ]

        # –û—Ü–µ–Ω–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º eval –º–æ–¥–µ–ª–∏
        evaluated_results = evaluate_dataset(
            dataset=dataset,
            system_responses=system_responses,
            model_name=EVAL_MODEL_NAME,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            temperature=TEMPERATURE,
            limit=limit,
            actual_contexts=actual_contexts,
        )

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report = generate_report(evaluated_results)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results_path = os.path.join(output_dir, f"{model_name.replace('/', '_')}_evaluation.csv")
        save_results(evaluated_results, results_path, report)

        elapsed_time = time.time() - start_time
        avg_score = report.get("summary", {}).get("overall_avg_score", 0.0)

        print(
            f"‚úÖ [{model_name}] –û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed_time:.1f}—Å. –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {avg_score:.4f}"
        )

        return {
            "model": model_name,
            "avg_score": avg_score,
            "evaluation_time": elapsed_time,
            "results_path": results_path,
            "responses_path": responses_path,
        }

    except Exception as e:
        print(f"‚ùå [{model_name}] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ: {e}")
        return {"model": model_name, "error": str(e), "evaluation_time": time.time() - start_time}


async def run_evaluations(
    models: list, dataset: pd.DataFrame, output_dir: str, limit: int, concurrency: int
):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞"""
    # –°–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π
    semaphore = asyncio.Semaphore(concurrency)

    async def run_with_semaphore(model):
        async with semaphore:
            return await evaluate_model(model, dataset, output_dir, limit)

    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    tasks = [run_with_semaphore(model) for model in models]

    # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏
    results = await asyncio.gather(*tasks, return_exceptions=True)

    return results


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π RAG")

    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    os.makedirs("research/logs", exist_ok=True)
    os.makedirs("research/data", exist_ok=True)

    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"research/results/evaluation_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)

    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        if not os.path.exists(TEST_DATASET_PATH):
            print(f"‚ùå –¢–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {TEST_DATASET_PATH}")
            print("–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: question, answer, context")
            return

        dataset = pd.read_csv(TEST_DATASET_PATH)
        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç: {len(dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")

        # –ü—Ä–µ–¥–ø–æ—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –≤—Å–µ—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        dataset = precompute_documents_for_all_questions(dataset, limit=LIMIT)

        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        print(f"\nüî¨ –ù–∞—á–∏–Ω–∞–µ–º –æ—Ü–µ–Ω–∫—É {len(MODELS_TO_EVALUATE)} –º–æ–¥–µ–ª–µ–π")
        evaluation_results = await run_evaluations(
            MODELS_TO_EVALUATE, dataset, output_dir, LIMIT, MAX_CONCURRENCY
        )

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        successful_results = [
            r for r in evaluation_results if isinstance(r, dict) and "error" not in r
        ]
        failed_results = [r for r in evaluation_results if isinstance(r, dict) and "error" in r]

        print("\nüìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏:")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ—Ü–µ–Ω–µ–Ω–æ: {len(successful_results)} –º–æ–¥–µ–ª–µ–π")
        print(f"‚ùå –û—à–∏–±–∫–∏: {len(failed_results)} –º–æ–¥–µ–ª–µ–π")

        if successful_results:
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ä–µ–¥–Ω–µ–π –æ—Ü–µ–Ω–∫–µ
            successful_results.sort(key=lambda x: x.get("avg_score", 0), reverse=True)

            print("\nüèÜ –†–µ–π—Ç–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π:")
            for i, result in enumerate(successful_results, 1):
                print(f"{i}. {result['model']}: {result['avg_score']:.4f}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç
        summary_path = os.path.join(output_dir, "evaluation_summary.json")
        summary = {
            "timestamp": timestamp,
            "dataset_size": len(dataset),
            "models_evaluated": len(MODELS_TO_EVALUATE),
            "successful_evaluations": len(successful_results),
            "failed_evaluations": len(failed_results),
            "results": evaluation_results,
        }

        with open(summary_path, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        print(f"\nüìÅ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_dir}")

    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        raise
    finally:
        # –û—á–∏—â–∞–µ–º –º–æ–¥–µ–ª–∏ –∏–∑ –ø–∞–º—è—Ç–∏
        cleanup_models()
        print("üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞")


def setup_model(model_name: str) -> bool:
    try:
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
        config["model"]["name"] = model_name
        get_llm(temperature=0.0)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")
        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
        return False


def evaluate_single_model(
    model_name: str,
    golden_data: list[dict[str, Any]],
    use_reranker: bool = True,
    top_search: int = 20,
    top_rerank: int = 5,
    limit: int | None = None,
) -> dict[str, Any]:
    print(f"\nüîÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å: {model_name}")

    if not setup_model(model_name):
        return {"model": model_name, "error": "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –º–æ–¥–µ–ª—å"}

    try:
        rag_chain = get_retrieval_chain(
            top_search=top_search, top_rerank=top_rerank, use_reranker=use_reranker
        )

        eval_data = []
        questions_to_process = golden_data[:limit] if limit else golden_data

        print(f"üìù –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç—ã –¥–ª—è {len(questions_to_process)} –≤–æ–ø—Ä–æ—Å–æ–≤...")

        for i, item in enumerate(questions_to_process):
            try:
                question = item["question"]
                expected_answer = item.get("answer", "")

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                cache_key = question
                if cache_key in _document_cache:
                    contexts = _document_cache[cache_key]
                    answer = rag_chain.invoke(question)
                    if isinstance(answer, dict):
                        answer = answer.get("answer", "")
                else:
                    result = rag_chain.invoke(question)

                    if isinstance(result, dict):
                        contexts = result.get("context", [])
                        answer = result.get("answer", "")
                        # –ö—ç—à–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                        _document_cache[cache_key] = contexts
                    else:
                        contexts = []
                        answer = str(result)

                if isinstance(contexts, str):
                    contexts = [contexts]

                eval_item = {
                    "question": question,
                    "answer": answer,
                    "expected_answer": expected_answer,
                    "contexts": contexts,
                }

                eval_data.append(eval_item)

                if (i + 1) % 5 == 0:
                    print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i + 1}/{len(questions_to_process)}")

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞ {i}: {e}")
                continue

        if not eval_data:
            return {"model": model_name, "error": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏"}

        print(f"üìä –û—Ü–µ–Ω–∫–∞ {len(eval_data)} –ø—Ä–∏–º–µ—Ä–æ–≤...")
        results = evaluate_batch(eval_data, metrics=["rag_triad", "bleurt", "cosine_similarity"])

        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        aggregated = {}
        numeric_metrics = []

        for result in results:
            for key, value in result.items():
                if isinstance(value, int | float) and key not in ["question", "answer"]:
                    if key not in numeric_metrics:
                        numeric_metrics.append(key)

        for metric in numeric_metrics:
            values = [r.get(metric, 0) for r in results if isinstance(r.get(metric), int | float)]
            if values:
                aggregated[f"avg_{metric}"] = sum(values) / len(values)

        return {"model": model_name, "total_examples": len(eval_data), **aggregated}

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
        return {"model": model_name, "error": str(e)}


if __name__ == "__main__":
    asyncio.run(main())
