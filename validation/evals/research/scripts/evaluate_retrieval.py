#!/usr/bin/env python
import argparse
import gc
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd
import torch
from langchain_community.embeddings import HuggingFaceEmbeddings
from sklearn.metrics import ndcg_score
from transformers import AutoModelForSequenceClassification, AutoTokenizer

PROJECT_ROOT = Path(__file__).resolve().parents[3]
EVALS_ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(EVALS_ROOT))

from core.vector_store import get_documents

EMBEDDING_MODELS = {
    "e5-large": "intfloat/multilingual-e5-large",
    "e5-base": "intfloat/multilingual-e5-base",
    "gte-large": "Alibaba-NLP/gte-multilingual-base",
    "labse": "sentence-transformers/LaBSE",
    "USER-bge-m3": "deepvk/USER-bge-m3",
    "jina-emb": "jinaai/jina-embeddings-v3",
    "KaLM": "HIT-TMG/KaLM-embedding-multilingual-mini-v1",
}

RERANKER_MODELS = {
    "gte-base": "Alibaba-NLP/gte-multilingual-reranker-base",
    "bge-v2-m3": "BAAI/bge-reranker-v2-m3",
    "jina-v2-base": "jinaai/jina-reranker-v2-base-multilingual",
}


def parse_args():
    parser = argparse.ArgumentParser(description="–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ—Ç—Ä–∏–≤–∞–ª–∞")

    parser.add_argument("--eval-dataset", type=str, required=True)
    parser.add_argument("--output-dir", type=str, default="research/results/retrieval")
    parser.add_argument("--k-values", nargs="+", type=int, default=[3, 5, 10])
    parser.add_argument("--retrieval-k", type=int, default=100)
    parser.add_argument("--device", type=str, default=None)
    parser.add_argument("--models", nargs="+", type=str, choices=list(EMBEDDING_MODELS.keys()))
    parser.add_argument("--rerankers", nargs="+", type=str, choices=list(RERANKER_MODELS.keys()))

    return parser.parse_args()


def setup_device():
    return "cuda" if torch.cuda.is_available() else "cpu"


def load_evaluation_dataset(file_path):
    if file_path.endswith(".jsonl"):
        eval_df = pd.read_json(file_path, lines=True)
    else:
        eval_df = pd.read_csv(file_path)

    required_columns = ["question", "relevant_doc_ids"]
    for col in required_columns:
        if col not in eval_df.columns:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞: {col}")

    return eval_df


def create_embedder(model_name: str, device: str):
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name, model_kwargs={"device": device, "trust_remote_code": True}
    )

    return embeddings


def create_reranker(model_name: str, device: str):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name, torch_dtype="auto", trust_remote_code=True
    ).to(device)
    model.eval()

    return tokenizer, model


def evaluate_retrieval_combination(
    eval_df: pd.DataFrame,
    documents: list[Any] | None,
    retrieval_model: str,
    reranker_model: str | None,
    device: str,
    k_list: list[int] | None = None,
    retrieval_k: int = 100,
) -> dict[str, Any]:
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ–¥–Ω—É –∫–æ–º–±–∏–Ω–∞—Ü–∏—é —Ä–µ—Ç—Ä–∏–≤–µ—Ä + —Ä–µ—Ä–∞–Ω–∫–µ—Ä"""
    if k_list is None:
        k_list = [1, 5, 10]

    print(f"üîÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º: {retrieval_model} + {reranker_model or '–±–µ–∑ —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞'}")

    if documents is None:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞")

    embedder = create_embedder(EMBEDDING_MODELS[retrieval_model], device)

    reranker_tokenizer = None
    reranker_model_obj = None
    if reranker_model:
        reranker_tokenizer, reranker_model_obj = create_reranker(
            RERANKER_MODELS[reranker_model], device
        )

    processed_queries = 0

    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ k
    metrics: dict[int, dict[str, list[float]]] = {
        k: {"recall_scores": [], "precision_scores": [], "ndcg_scores": [], "acc_scores": []}
        for k in k_list
    }

    for idx, row in eval_df.iterrows():
        try:
            query = row["question"]
            relevant_doc_ids = row["relevant_doc_ids"]

            if isinstance(relevant_doc_ids, str):
                relevant_doc_ids = eval(relevant_doc_ids)
            relevant_doc_ids = set(map(str, relevant_doc_ids))

            # –ü–æ–∏—Å–∫ —Å —ç–º–±–µ–¥–¥–µ—Ä–æ–º
            doc_contents = [doc.page_content for doc in documents]
            query_embedding = embedder.embed_query(query)
            doc_embeddings = embedder.embed_documents(doc_contents[:retrieval_k])

            query_embedding = np.array(query_embedding).reshape(1, -1)
            doc_embeddings = np.array(doc_embeddings)

            similarities = np.dot(query_embedding, doc_embeddings.T).flatten()
            top_indices = np.argsort(similarities)[::-1][:retrieval_k]

            retrieved_docs = [documents[i] for i in top_indices]

            # –†–µ—Ä–∞–Ω–∫–∏–Ω–≥
            if reranker_model and reranker_tokenizer and reranker_model_obj:
                inputs = [f"{query} [SEP] {doc.page_content}" for doc in retrieved_docs]

                with torch.no_grad():
                    tokenized = reranker_tokenizer(
                        inputs, padding=True, truncation=True, return_tensors="pt", max_length=2048
                    )
                    tokenized = {k: v.to(device) for k, v in tokenized.items()}
                    outputs = reranker_model_obj(**tokenized)
                    scores = outputs.logits.cpu().numpy().flatten()

                rerank_indices = np.argsort(scores)[::-1]
                retrieved_docs = [retrieved_docs[i] for i in rerank_indices]

            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ k
            for k in k_list:
                top_k_docs = retrieved_docs[:k]
                retrieved_doc_ids = set()

                for doc in top_k_docs:
                    if hasattr(doc, "metadata") and "doc_id" in doc.metadata:
                        retrieved_doc_ids.add(str(doc.metadata["doc_id"]))

                recall = (
                    len(relevant_doc_ids & retrieved_doc_ids) / len(relevant_doc_ids)
                    if relevant_doc_ids
                    else 0
                )
                precision = (
                    len(relevant_doc_ids & retrieved_doc_ids) / len(retrieved_doc_ids)
                    if retrieved_doc_ids
                    else 0
                )

                # NDCG
                relevance_scores = [
                    1 if str(doc.metadata.get("doc_id", "")) in relevant_doc_ids else 0
                    for doc in top_k_docs
                ]
                if len(relevance_scores) > 0:
                    ideal_scores = sorted(
                        [1] * len(relevant_doc_ids) + [0] * (k - len(relevant_doc_ids)),
                        reverse=True,
                    )[:k]
                    ndcg = (
                        ndcg_score([ideal_scores], [relevance_scores])
                        if sum(ideal_scores) > 0
                        else 0
                    )
                else:
                    ndcg = 0

                # Accuracy@k
                accuracy = 1 if any(score > 0 for score in relevance_scores) else 0

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                metrics[k]["recall_scores"].append(recall)
                metrics[k]["precision_scores"].append(precision)
                metrics[k]["ndcg_scores"].append(ndcg)
                metrics[k]["acc_scores"].append(accuracy)

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞ {idx}: {e}")
            continue

        processed_queries += 1
        if processed_queries % 10 == 0:
            print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_queries}")

    # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    result: dict[str, str | int | float | None] = {
        "retriever": retrieval_model,
        "reranker": reranker_model,
        "processed_queries": processed_queries,
    }

    for k in k_list:
        if metrics[k]["recall_scores"]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
            result.update(
                {
                    f"recall@{k}": float(np.mean(metrics[k]["recall_scores"])),
                    f"precision@{k}": float(np.mean(metrics[k]["precision_scores"])),
                    f"ndcg@{k}": float(np.mean(metrics[k]["ndcg_scores"])),
                    f"accuracy@{k}": float(np.mean(metrics[k]["acc_scores"])),
                }
            )
        else:
            result.update(
                {f"recall@{k}": 0.0, f"precision@{k}": 0.0, f"ndcg@{k}": 0.0, f"accuracy@{k}": 0.0}
            )

    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {processed_queries}")

    # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
    del embedder
    if reranker_model_obj:
        del reranker_model_obj, reranker_tokenizer
    torch.cuda.empty_cache()
    gc.collect()

    return result


def run_evaluation(args):
    """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π –æ—Ü–µ–Ω–∫–∏"""

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    device = args.device or setup_device()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    eval_df = load_evaluation_dataset(args.eval_dataset)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    retrieval_models = args.models or list(EMBEDDING_MODELS.keys())
    reranker_models = args.rerankers or list(RERANKER_MODELS.keys())

    print(f"üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º {len(retrieval_models)} —Ä–µ—Ç—Ä–∏–≤–µ—Ä–æ–≤ √ó {len(reranker_models)} —Ä–µ—Ä–∞–Ω–∫–µ—Ä–æ–≤")
    print(f"üìä –ú–µ—Ç—Ä–∏–∫–∏: K = {args.k_values}")

    # –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏
    results = []
    total_combinations = len(retrieval_models) * len(reranker_models)

    for i, retrieval_model in enumerate(retrieval_models):
        for j, reranker_model in enumerate(reranker_models):
            combination_num = i * len(reranker_models) + j + 1
            print(f"\n{'=' * 60}")
            print(f"–ö–æ–º–±–∏–Ω–∞—Ü–∏—è {combination_num}/{total_combinations}")
            print(f"{'=' * 60}")

            try:
                result = evaluate_retrieval_combination(
                    eval_df=eval_df,
                    documents=get_documents(),
                    retrieval_model=retrieval_model,
                    reranker_model=reranker_model,
                    device=device,
                    k_list=args.k_values,
                    retrieval_k=args.retrieval_k,
                )
                results.append(result)

                # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                if results:
                    interim_df = pd.DataFrame(results)
                    os.makedirs(args.output_dir, exist_ok=True)
                    interim_path = os.path.join(args.output_dir, "interim_results.csv")
                    interim_df.to_csv(interim_path, index=False)

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ {retrieval_model} + {reranker_model}: {e}")
                continue

    return results


def save_results(results: list, output_dir: str, k_values: list):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    if not results:
        print("‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return

    os.makedirs(output_dir, exist_ok=True)

    # –°–æ–∑–¥–∞–µ–º DataFrame
    results_df = pd.DataFrame(results)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # CSV
    csv_path = os.path.join(output_dir, f"retrieval_evaluation_{timestamp}.csv")
    results_df.to_csv(csv_path, index=False)
    print(f"üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {csv_path}")

    # JSON
    json_path = os.path.join(output_dir, f"retrieval_evaluation_{timestamp}.json")
    results_df.to_json(json_path, orient="records", indent=2, force_ascii=False)
    print(f"üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {json_path}")

    # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
    print("\nüìä –¢–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º:")

    for k in k_values:
        print(f"\nüèÜ –¢–æ–ø-3 –ø–æ Recall@{k}:")
        top_recall = results_df.nlargest(3, f"recall@{k}")[["retriever", "reranker", f"recall@{k}"]]
        print(top_recall.to_string(index=False))

        print(f"\nüèÜ –¢–æ–ø-3 –ø–æ NDCG@{k}:")
        top_ndcg = results_df.nlargest(3, f"ndcg@{k}")[["retriever", "reranker", f"ndcg@{k}"]]
        print(top_ndcg.to_string(index=False))


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    args = parse_args()

    print("üî¨ –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ—Ç—Ä–∏–≤–∞–ª–∞")
    print(f"–î–∞—Ç–∞—Å–µ—Ç: {args.eval_dataset}")
    print(f"–í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {args.output_dir}")

    try:
        # –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏
        results = run_evaluation(args)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        save_results(results, args.output_dir, args.k_values)

        print(f"\n‚úÖ –û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {args.output_dir}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        raise


if __name__ == "__main__":
    main()
