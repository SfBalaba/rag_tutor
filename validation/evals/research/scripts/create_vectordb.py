import argparse
import os
import sys
from pathlib import Path

import pandas as pd
from langchain.text_splitter import (
    MarkdownTextSplitter,
    RecursiveCharacterTextSplitter,
    SentenceTransformersTokenTextSplitter,
)
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from markdownify import markdownify
from tqdm import tqdm

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ core –º–æ–¥—É–ª–µ–π –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ sys.path
from core.config import config  # noqa: E402
from core.vector_store import get_embedding_model  # noqa: E402


def preprocess_article(content: str) -> str:
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: HTML -> Markdown + –æ—á–∏—Å—Ç–∫–∞"""
    if not content or pd.isna(content):
        return ""

    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è HTML –≤ Markdown
    markdown_content = markdownify(content, heading_style="ATX")

    # –û—á–∏—Å—Ç–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–µ–≥–æ–≤ –¢–ñ
    special_tags = [
        "[author]",
        "[/author]",
        "[img]",
        "[/img]",
        "[nobr]",
        "[/nobr]",
        "[quote]",
        "[/quote]",
        "[video]",
        "[/video]",
        "[audio]",
        "[/audio]",
    ]

    for tag in special_tags:
        markdown_content = markdown_content.replace(tag, "")

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
    markdown_content = markdown_content.replace("\u00a0", " ")  # –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–π –ø—Ä–æ–±–µ–ª
    markdown_content = "\n".join(line.strip() for line in markdown_content.split("\n"))

    return markdown_content.strip()


def create_chunker(chunker_type: str, chunk_size: int, chunk_overlap: int):
    """–°–æ–∑–¥–∞–µ—Ç —á–∞–Ω–∫–µ—Ä —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞"""
    if chunker_type == "markdown":
        return MarkdownTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    elif chunker_type == "recursive":
        return RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap, separators=["\n\n", "\n", " ", ""]
        )
    elif chunker_type == "sentence":
        return RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=[". ", "! ", "? ", "\n\n", "\n", " "],
        )
    elif chunker_type == "token":
        return SentenceTransformersTokenTextSplitter(
            chunk_overlap=chunk_overlap,
            tokens_per_chunk=chunk_size // 4,  # –ø—Ä–∏–º–µ—Ä–Ω–æ 4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
        )
    elif chunker_type == "hierarchical":
        # –î–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ: —Å–Ω–∞—á–∞–ª–∞ –∫—Ä—É–ø–Ω—ã–µ —á–∞–Ω–∫–∏, –ø–æ—Ç–æ–º –º–µ–ª–∫–∏–µ
        return HierarchicalChunker(
            primary_size=chunk_size * 2, secondary_size=chunk_size, overlap=chunk_overlap
        )
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —á–∞–Ω–∫–µ—Ä–∞: {chunker_type}")


class HierarchicalChunker:
    """–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫–µ—Ä: –¥–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ"""

    def __init__(self, primary_size: int = 2000, secondary_size: int = 600, overlap: int = 100):
        self.primary_splitter = MarkdownTextSplitter(chunk_size=primary_size, chunk_overlap=overlap)
        self.secondary_splitter = MarkdownTextSplitter(
            chunk_size=secondary_size, chunk_overlap=overlap
        )

    def split_documents(self, documents: list[Document]) -> list[Document]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏"""
        result = []

        for doc in documents:
            # –ü–µ—Ä–≤—ã–π —É—Ä–æ–≤–µ–Ω—å: –∫—Ä—É–ø–Ω—ã–µ —á–∞–Ω–∫–∏
            primary_chunks = self.primary_splitter.split_documents([doc])

            for i, primary_chunk in enumerate(primary_chunks):
                # –í—Ç–æ—Ä–æ–π —É—Ä–æ–≤–µ–Ω—å: –º–µ–ª–∫–∏–µ —á–∞–Ω–∫–∏ –∏–∑ –∫—Ä—É–ø–Ω—ã—Ö
                secondary_chunks = self.secondary_splitter.split_documents([primary_chunk])

                for j, secondary_chunk in enumerate(secondary_chunks):
                    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–± –∏–µ—Ä–∞—Ä—Ö–∏–∏
                    secondary_chunk.metadata.update(
                        {
                            "primary_chunk_id": i,
                            "secondary_chunk_id": j,
                            "chunk_type": "hierarchical",
                        }
                    )
                    result.append(secondary_chunk)

        return result


def process_data_to_documents(df: pd.DataFrame, text_splitter) -> list[Document]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç DataFrame –≤ —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    documents = []

    for idx, row in tqdm(df.iterrows(), total=len(df), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç–µ–π"):
        # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
        title = str(row.get("title", "")).strip()
        subtitle = str(row.get("subtitle", "")).strip()
        content = str(row.get("content_raw", "")).strip()

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
        full_text_parts = []
        if title and title != "nan":
            full_text_parts.append(f"# {title}")
        if subtitle and subtitle != "nan":
            full_text_parts.append(f"## {subtitle}")
        if content and content != "nan":
            full_text_parts.append(content)

        full_text = "\n\n".join(full_text_parts)

        if not full_text.strip():
            continue

        # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        doc = Document(
            page_content=full_text,
            metadata={
                "article_id": str(row.get("id", idx)),
                "title": title,
                "subtitle": subtitle,
                "rubrics": str(row.get("rubrics", "")),
                "tags": str(row.get("tags", "")),
                "author": str(row.get("author", "")),
                "source_row": idx,
            },
        )

        documents.append(doc)

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
    print(f"–†–∞–∑–±–∏–µ–Ω–∏–µ {len(documents)} —Å—Ç–∞—Ç–µ–π –Ω–∞ —á–∞–Ω–∫–∏...")
    chunks = text_splitter.split_documents(documents)

    # –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ ID –¥–ª—è —á–∞–Ω–∫–æ–≤
    for i, chunk in enumerate(chunks):
        chunk.metadata["chunk_id"] = f"chunk_{i}"

    print(f"–°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
    return chunks


def create_vectorstore(documents: list[Document], embedding_model) -> FAISS:
    """–°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
    print("–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞...")

    # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
    vectorstore = FAISS.from_documents(documents, embedding_model)

    print(f"–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ–∑–¥–∞–Ω–æ —Å {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
    return vectorstore


def save_vectorstore(vectorstore: FAISS, output_path: str):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
    os.makedirs(output_path, exist_ok=True)
    vectorstore.save_local(output_path)
    print(f"–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {output_path}")


def parse_args():
    parser = argparse.ArgumentParser(description="–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¢–ñ")

    parser.add_argument(
        "--input", type=str, required=True, help="–ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É CSV —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏ –¢–ñ"
    )
    parser.add_argument(
        "--output", type=str, required=True, help="–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î"
    )
    parser.add_argument(
        "--embedding-model",
        type=str,
        default=config["embedding"]["model"],
        help="–ú–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤",
    )
    parser.add_argument(
        "--chunker-type",
        type=str,
        choices=["markdown", "recursive", "sentence", "token", "hierarchical"],
        default="markdown",
        help="–¢–∏–ø —á–∞–Ω–∫–µ—Ä–∞ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞",
    )
    parser.add_argument(
        "--chunk-size", type=int, default=1200, help="–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞"
    )
    parser.add_argument(
        "--chunk-overlap", type=int, default=0, help="–ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —á–∞–Ω–∫–æ–≤ –ø—Ä–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞"
    )
    parser.add_argument(
        "--filter-ugc",
        action="store_true",
        default=True,
        help="–§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏–∑ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ (UGC)",
    )
    parser.add_argument("--separator", type=str, default=";", help="–†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤ CSV —Ñ–∞–π–ª–µ")

    return parser.parse_args()


def main():
    args = parse_args()

    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {args.input}...")
    df = pd.read_csv(args.input, sep=args.separator)
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")

    # –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å–∏ –±–µ–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    initial_count = len(df)
    df = df.dropna(subset=["content_raw"])
    print(f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –ø—É—Å—Ç—ã—Ö –∑–∞–ø–∏—Å–µ–π: {len(df)} ({initial_count - len(df)} —É–¥–∞–ª–µ–Ω–æ)")

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è UGC –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
    if args.filter_ugc and "rubrics" in df.columns:
        initial_count = len(df)
        df = df[df["rubrics"] != "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∏–∑ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞"]
        print(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ UGC: {len(df)} ({initial_count - len(df)} —É–¥–∞–ª–µ–Ω–æ)")

    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    print("–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç–µ–π...")
    df["content_raw"] = df["content_raw"].apply(preprocess_article)

    # –°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–µ—Ä–∞
    print(f"–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–µ—Ä–∞ —Ç–∏–ø–∞: {args.chunker_type}")
    text_splitter = create_chunker(args.chunker_type, args.chunk_size, args.chunk_overlap)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    documents = process_data_to_documents(df, text_splitter)

    # –°–æ–∑–¥–∞–Ω–∏–µ embedding –º–æ–¥–µ–ª–∏
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ embedding –º–æ–¥–µ–ª–∏: {args.embedding_model}")
    embedding_model = get_embedding_model(model_name=args.embedding_model)

    # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    vectorstore = create_vectorstore(documents, embedding_model)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    save_vectorstore(vectorstore, args.output)

    print("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞!")
    print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   - –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(df)}")
    print(f"   - –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(documents)}")
    print(f"   - –¢–∏–ø —á–∞–Ω–∫–µ—Ä–∞: {args.chunker_type}")
    print(f"   - –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {args.chunk_size}")
    print(f"   - –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ: {args.chunk_overlap}")
    print(f"   - Embedding –º–æ–¥–µ–ª—å: {args.embedding_model}")
    print(f"   - –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {args.output}")


if __name__ == "__main__":
    main()
