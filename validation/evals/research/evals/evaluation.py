import asyncio
import gc
import json
import os
from contextlib import contextmanager
from typing import Any

import numpy as np
import pandas as pd
import torch
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm

try:
    from bleurt_pytorch import BleurtConfig, BleurtForSequenceClassification, BleurtTokenizer

    BLEURT_AVAILABLE = True
except ImportError:
    BLEURT_AVAILABLE = False

try:
    from deepeval.metrics import (
        AnswerRelevancyMetric,
        ContextualRelevancyMetric,
        FaithfulnessMetric,
        GEval,
    )
    from deepeval.test_case import LLMTestCase, LLMTestCaseParams

    DEEPEVAL_AVAILABLE = True
except ImportError:
    DEEPEVAL_AVAILABLE = False

from core.config import config
from core.llm import get_llm
from core.llm.deepeval_adapter import create_deepeval_adapter


@contextmanager
def gpu_memory_manager():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    yield
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()


_bleurt_model = None
_bleurt_tokenizer = None


def get_bleurt_model():
    global _bleurt_model
    if not BLEURT_AVAILABLE or _bleurt_model is not None:
        return _bleurt_model

    with gpu_memory_manager():
        device = (
            "cuda:1"
            if torch.cuda.device_count() > 1
            else ("cuda:0" if torch.cuda.is_available() else "cpu")
        )
        _bleurt_model = BleurtForSequenceClassification.from_pretrained("lucadiliello/BLEURT-20")
        _bleurt_model.eval()
        if torch.cuda.is_available():
            if device == "cuda:1":
                torch.cuda.set_per_process_memory_fraction(0.5, device=1)
            _bleurt_model = _bleurt_model.to(device)
    return _bleurt_model


def get_bleurt_tokenizer():
    global _bleurt_tokenizer
    if not BLEURT_AVAILABLE or _bleurt_tokenizer is not None:
        return _bleurt_tokenizer
    _bleurt_tokenizer = BleurtTokenizer.from_pretrained("lucadiliello/BLEURT-20")
    return _bleurt_tokenizer


def split_docs(context):
    if isinstance(context, list):
        return context
    separator = config.get("docs_separator", "\n\n-----")
    return [doc.strip() for doc in context.split(separator) if doc.strip()]


def create_test_cases(
    dataset: pd.DataFrame,
    system_responses: pd.DataFrame | None = None,
    limit: int | None = None,
    actual_contexts: list[str] | None = None,
) -> list["LLMTestCase"]:
    """–°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏"""
    if not DEEPEVAL_AVAILABLE:
        raise ImportError("DeepEval –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –í—ã–ø–æ–ª–Ω–∏—Ç–µ: pip install deepeval")

    if limit is not None and limit < len(dataset):
        if system_responses is not None:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤ –ø—Ä–∏ –≤—ã–±–æ—Ä–∫–µ
            dataset = dataset.sample(limit, random_state=42).reset_index(drop=True)
            system_responses = system_responses.reset_index(drop=True)

            # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ —Ä–∞–∑–º–µ—Ä—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç
            if len(dataset) != len(system_responses):
                min_len = min(len(dataset), len(system_responses))
                dataset = dataset.iloc[:min_len]
                system_responses = system_responses.iloc[:min_len]

            # –ï—Å–ª–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã, —Ç–∞–∫–∂–µ –æ–±—Ä–µ–∑–∞–µ–º –∏—Ö
            if actual_contexts is not None:
                if len(actual_contexts) != len(dataset):
                    min_len = min(len(dataset), len(actual_contexts))
                    actual_contexts = actual_contexts[:min_len]
        else:
            dataset = dataset.sample(limit, random_state=42).reset_index(drop=True)

    test_cases = []
    for i, (_, row) in enumerate(
        tqdm(dataset.iterrows(), total=len(dataset), desc="–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤")
    ):
        # –ï—Å–ª–∏ –µ—Å—Ç—å –æ—Ç–≤–µ—Ç—ã —Å–∏—Å—Ç–µ–º—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –¥–ª—è actual_output
        if system_responses is not None:
            actual_output = system_responses.iloc[i].get(
                "system_answer", system_responses.iloc[i].get("answer", "")
            )
        else:
            # –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–≤–µ—Ç –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
            actual_output = row["answer"]

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è retrieval_context
        if actual_contexts is not None:
            context_to_use = actual_contexts[i]
        else:
            context_to_use = row["context"]

        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Å–ª—É—á–∞–π
        test_case = LLMTestCase(
            input=row["question"],
            actual_output=actual_output,
            expected_output=row["answer"],
            retrieval_context=split_docs(context_to_use),
        )
        test_cases.append(test_case)

    return test_cases


# –§—É–Ω–∫—Ü–∏—è —É–¥–∞–ª–µ–Ω–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è create_deepeval_adapter –∏–∑ core.llm.deepeval_adapter


def evaluate_dataset(
    dataset: pd.DataFrame,
    system_responses: pd.DataFrame | None = None,
    model_name: str | None = None,
    temperature: float = 0.0,
    limit: int | None = None,
    actual_contexts: list[str] | None = None,
) -> pd.DataFrame:
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""

    if model_name is None:
        model_name = config["model"]["name"]

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏
    test_cases = create_test_cases(dataset, system_responses, limit, actual_contexts)

    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (–µ—Å–ª–∏ DeepEval –¥–æ—Å—Ç—É–ø–µ–Ω)
    eval_model = None
    if DEEPEVAL_AVAILABLE:
        eval_model = create_deepeval_adapter(model_name, temperature)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è BLEURT –∏ –∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
    references = [test_case.expected_output for test_case in test_cases]
    candidates = [test_case.actual_output for test_case in test_cases]

    # –†–∞—Å—á–µ—Ç BLEURT –æ—Ü–µ–Ω–æ–∫
    bleurt_scores = calculate_bleurt_score(references, candidates)

    # –†–∞—Å—á–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ (batch –≤–µ—Ä—Å–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏)
    cosine_scores = calculate_cosine_similarity_batch(references, candidates)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ DeepEval (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
    deepeval_metrics = []
    if DEEPEVAL_AVAILABLE and eval_model:
        deepeval_metrics = [
            FaithfulnessMetric(threshold=0.5, model=eval_model),
            AnswerRelevancyMetric(threshold=0.5, model=eval_model),
            ContextualRelevancyMetric(threshold=0.5, model=eval_model),
            GEval(
                name="Correctness",
                criteria="–û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ '—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥' –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –Ω–∞ –æ—Å–Ω–æ–≤–µ '–æ–∂–∏–¥–∞–µ–º–æ–≥–æ –≤—ã–≤–æ–¥–∞'.",
                evaluation_params=[
                    LLMTestCaseParams.ACTUAL_OUTPUT,
                    LLMTestCaseParams.EXPECTED_OUTPUT,
                ],
                threshold=0.5,
                model=eval_model,
            ),
        ]

    # –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
    metric_error_counts = (
        {metric.__class__.__name__.replace("Metric", ""): 0 for metric in deepeval_metrics}
        if deepeval_metrics
        else {}
    )

    results = []
    for i, test_case in enumerate(tqdm(test_cases, desc="–û—Ü–µ–Ω–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤")):
        metric_scores = {}

        # –û—Ü–µ–Ω–∫–∞ DeepEval –º–µ—Ç—Ä–∏–∫–∞–º–∏
        for metric in deepeval_metrics:
            metric_name = metric.__class__.__name__.replace("Metric", "")
            try:
                metric.measure(test_case)
                metric_scores[metric_name] = metric.score
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –º–µ—Ç—Ä–∏–∫–µ {metric_name}: {e}")
                metric_scores[metric_name] = 0.0
                metric_error_counts[metric_name] += 1

        # –î–æ–±–∞–≤–ª—è–µ–º BLEURT –∏ –∫–æ—Å–∏–Ω—É—Å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        metric_scores["BLEURT"] = bleurt_scores[i]
        metric_scores["CosineSimilarity"] = cosine_scores[i]

        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é –æ—Ü–µ–Ω–∫—É (—Å—Ä–µ–¥–Ω–µ–µ –ø–æ –≤—Å–µ–º –º–µ—Ç—Ä–∏–∫–∞–º)
        all_scores = list(metric_scores.values())
        avg_score = np.mean(all_scores) if all_scores else 0.0

        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        result = {
            "question": test_case.input,
            "expected_output": test_case.expected_output,
            "actual_output": test_case.actual_output,
            "avg_score": avg_score,
            **metric_scores,
        }

        results.append(result)

    # –í—ã–≤–æ–¥–∏–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É –æ—à–∏–±–æ–∫
    if metric_error_counts:
        print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫ –º–µ—Ç—Ä–∏–∫:")
        for metric_name, error_count in metric_error_counts.items():
            if error_count > 0:
                print(f"  {metric_name}: {error_count}/{len(test_cases)} –æ—à–∏–±–æ–∫")

    return pd.DataFrame(results)


def generate_report(evaluated_df: pd.DataFrame) -> dict[str, Any]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –æ—Ü–µ–Ω–∫–∏"""
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ (–∏—Å–∫–ª—é—á–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è)
    exclude_columns = [
        "question",
        "expected_output",
        "actual_output",
        "system_answer",
        "golden_answer",
        "context",
        "chunk_ids",
    ]
    metric_columns = [col for col in evaluated_df.columns if col not in exclude_columns]

    report = {}

    print("\nüìä –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ú–ï–¢–†–ò–ö–ê–ú:")
    print(f"   –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(evaluated_df)}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–µ
    for metric in metric_columns:
        if metric in evaluated_df.columns:
            all_values = evaluated_df[metric]
            scores = all_values.dropna().tolist()
            none_count = all_values.isna().sum()

            print(f"\n   {metric}:")
            print(f"     –£—Å–ø–µ—à–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫: {len(scores)}/{len(evaluated_df)}")
            print(f"     None –∑–Ω–∞—á–µ–Ω–∏–π: {none_count}")

            if scores:
                mean_val = float(np.mean(scores))
                std_val = float(np.std(scores))
                min_val = float(np.min(scores))
                max_val = float(np.max(scores))
                median_val = float(np.median(scores))

                report[metric] = {
                    "mean": mean_val,
                    "std": std_val,
                    "min": min_val,
                    "max": max_val,
                    "median": median_val,
                    "count": len(scores),
                    "none_count": int(none_count),
                }

                print(f"     –°—Ä–µ–¥–Ω–µ–µ: {mean_val:.4f} ¬± {std_val:.4f}")
                print(f"     –î–∏–∞–ø–∞–∑–æ–Ω: [{min_val:.4f}, {max_val:.4f}]")
                print(f"     –ú–µ–¥–∏–∞–Ω–∞: {median_val:.4f}")

                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
                if metric in ["ContextualRelevancy", "Faithfulness"]:
                    print(f"     üîç –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ {metric}:")
                    if std_val > 0:
                        cv = (std_val / mean_val * 100) if mean_val > 0 else 0
                        print(f"       –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏: {cv:.1f}%")
                    if len(scores) > 5:
                        print(f"       –ü–µ—Ä–≤—ã–µ 5 –∑–Ω–∞—á–µ–Ω–∏–π: {scores[:5]}")
                        print(f"       –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 –∑–Ω–∞—á–µ–Ω–∏–π: {scores[-5:]}")
            else:
                report[metric] = {
                    "mean": 0.0,
                    "std": 0.0,
                    "min": 0.0,
                    "max": 0.0,
                    "median": 0.0,
                    "count": 0,
                    "none_count": int(none_count),
                }

    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    report["summary"] = {"total_examples": len(evaluated_df), "metrics_count": len(metric_columns)}

    # –ï—Å–ª–∏ –µ—Å—Ç—å avg_score, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –≤ —Å–≤–æ–¥–∫—É
    if "avg_score" in evaluated_df.columns:
        avg_scores = evaluated_df["avg_score"].dropna()
        if len(avg_scores) > 0:
            overall_avg = float(avg_scores.mean())
            report["summary"]["overall_avg_score"] = overall_avg
            print(f"\n   üìà –û–±—â–∞—è —Å—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {overall_avg:.4f}")

    return report


def filter_best_examples(evaluated_df: pd.DataFrame, threshold: float = 0.5) -> pd.DataFrame:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç –ª—É—á—à–∏–µ –ø—Ä–∏–º–µ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ avg_score"""
    if "avg_score" not in evaluated_df.columns:
        print("‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ 'avg_score' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç")
        return evaluated_df

    filtered_df = evaluated_df[evaluated_df["avg_score"] >= threshold]
    print(f"üìä –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(filtered_df)}/{len(evaluated_df)} –ø—Ä–∏–º–µ—Ä–æ–≤ (–ø–æ—Ä–æ–≥: {threshold})")

    return filtered_df


def save_results(dataset: pd.DataFrame, output_path: str, report: dict[str, Any] | None = None):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏"""
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    dataset.to_csv(output_path, index=False)
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç –µ—Å–ª–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω
    if report:
        report_path = output_path.replace(".csv", "_report.json")
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")


# –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ —Ñ—É–Ω–∫—Ü–∏–π
async def calculate_bleurt_score_async(references: list[str], candidates: list[str]) -> list[float]:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç BLEURT –æ—Ü–µ–Ω–∫—É"""
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, calculate_bleurt_score, references, candidates)


async def calculate_cosine_similarity_async(texts1: list[str], texts2: list[str]) -> list[float]:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å"""
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, calculate_cosine_similarity_batch, texts1, texts2)


async def evaluate_dataset_async(
    dataset: pd.DataFrame,
    system_responses: pd.DataFrame | None = None,
    model_name: str | None = None,
    temperature: float = 0.0,
    limit: int | None = None,
    max_concurrency: int = 6,
    actual_contexts: list[str] | None = None,
) -> pd.DataFrame:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è evaluate_dataset —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –º–µ—Ç—Ä–∏–∫"""

    if model_name is None:
        model_name = config["model"]["name"]

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏
    test_cases = create_test_cases(dataset, system_responses, limit, actual_contexts)

    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (–µ—Å–ª–∏ DeepEval –¥–æ—Å—Ç—É–ø–µ–Ω)
    eval_model = None
    if DEEPEVAL_AVAILABLE:
        eval_model = create_deepeval_adapter(model_name, temperature)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è BLEURT –∏ –∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
    references = [test_case.expected_output for test_case in test_cases]
    candidates = [test_case.actual_output for test_case in test_cases]

    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç BLEURT –∏ –∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫
    bleurt_task = asyncio.create_task(calculate_bleurt_score_async(references, candidates))
    cosine_task = asyncio.create_task(calculate_cosine_similarity_async(references, candidates))

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ DeepEval (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
    deepeval_metrics = []
    if DEEPEVAL_AVAILABLE and eval_model:
        deepeval_metrics = [
            FaithfulnessMetric(threshold=0.5, model=eval_model),
            AnswerRelevancyMetric(threshold=0.5, model=eval_model),
            ContextualRelevancyMetric(threshold=0.5, model=eval_model),
            GEval(
                name="Correctness",
                criteria="–û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ '—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥' –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –Ω–∞ –æ—Å–Ω–æ–≤–µ '–æ–∂–∏–¥–∞–µ–º–æ–≥–æ –≤—ã–≤–æ–¥–∞'.",
                evaluation_params=[
                    LLMTestCaseParams.ACTUAL_OUTPUT,
                    LLMTestCaseParams.EXPECTED_OUTPUT,
                ],
                threshold=0.5,
                model=eval_model,
            ),
        ]

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ LLM
    semaphore = asyncio.Semaphore(max_concurrency)

    async def process_test_case(i: int, test_case) -> tuple[int, dict[str, Any]]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —Ç–µ—Å—Ç–æ–≤—ã–π —Å–ª—É—á–∞–π —Å–æ –≤—Å–µ–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        async with semaphore:
            metric_scores = {}

            # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –≤—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            for metric in deepeval_metrics:
                metric_name = metric.__class__.__name__.replace("Metric", "")
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ a_measure
                    if hasattr(metric, "a_measure"):
                        await metric.a_measure(test_case)
                    else:
                        # –ï—Å–ª–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –Ω–µ—Ç, –∑–∞–ø—É—Å–∫–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤ executor
                        loop = asyncio.get_event_loop()
                        await loop.run_in_executor(None, metric.measure, test_case)

                    metric_scores[metric_name] = metric.score
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –º–µ—Ç—Ä–∏–∫–µ {metric_name}: {e}")
                    metric_scores[metric_name] = 0.0

            return i, metric_scores

    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    print(
        f"üöÄ –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ {len(test_cases)} –ø—Ä–∏–º–µ—Ä–æ–≤ (concurrency: {max_concurrency})"
    )

    tasks = [process_test_case(i, test_case) for i, test_case in enumerate(test_cases)]
    metric_results = await asyncio.gather(*tasks, return_exceptions=True)

    # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥—Ä—É–≥–∏—Ö –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
    bleurt_scores = await bleurt_task
    cosine_scores = await cosine_task

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = []
    metric_error_counts = {
        metric.__class__.__name__.replace("Metric", ""): 0 for metric in deepeval_metrics
    }

    for result in metric_results:
        if isinstance(result, Exception):
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Å–ª—É—á–∞—è: {result}")
            continue

        i, metric_scores = result
        test_case = test_cases[i]

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—à–∏–±–∫–∏
        for metric_name, score in metric_scores.items():
            if score == 0.0:  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ 0.0 –æ–∑–Ω–∞—á–∞–µ—Ç –æ—à–∏–±–∫—É
                metric_error_counts[metric_name] += 1

        # –î–æ–±–∞–≤–ª—è–µ–º BLEURT –∏ –∫–æ—Å–∏–Ω—É—Å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        metric_scores["BLEURT"] = bleurt_scores[i]
        metric_scores["CosineSimilarity"] = cosine_scores[i]

        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é –æ—Ü–µ–Ω–∫—É (—Å—Ä–µ–¥–Ω–µ–µ –ø–æ –≤—Å–µ–º –º–µ—Ç—Ä–∏–∫–∞–º)
        all_scores = list(metric_scores.values())
        avg_score = np.mean(all_scores) if all_scores else 0.0

        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        result_item = {
            "question": test_case.input,
            "expected_output": test_case.expected_output,
            "actual_output": test_case.actual_output,
            "avg_score": avg_score,
            **metric_scores,
        }

        results.append(result_item)

    # –í—ã–≤–æ–¥–∏–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É –æ—à–∏–±–æ–∫
    if metric_error_counts:
        print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫ –º–µ—Ç—Ä–∏–∫ (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è):")
        for metric_name, error_count in metric_error_counts.items():
            if error_count > 0:
                print(f"  {metric_name}: {error_count}/{len(test_cases)} –æ—à–∏–±–æ–∫")

    return pd.DataFrame(results)


def cleanup_models():
    """–û—á–∏—â–∞–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–∞–º—è—Ç–∏"""
    global _bleurt_model, _bleurt_tokenizer

    if _bleurt_model is not None:
        del _bleurt_model
        _bleurt_model = None

    if _bleurt_tokenizer is not None:
        del _bleurt_tokenizer
        _bleurt_tokenizer = None

    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    gc.collect()
    print("‚úÖ –ú–æ–¥–µ–ª–∏ –æ—á–∏—â–µ–Ω—ã –∏–∑ –ø–∞–º—è—Ç–∏")


def stop():
    """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç –≤—Å–µ —Ä–µ—Å—É—Ä—Å—ã –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏"""
    cleanup_models()

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

    gc.collect()
    print("‚úÖ –í—Å–µ —Ä–µ—Å—É—Ä—Å—ã –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω—ã")


class OpenRouterAdapter:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è DeepEval (—É—Å—Ç–∞—Ä–µ–≤—à–∏–π, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ create_deepeval_adapter)"""

    def __init__(self, model_name: str | None = None):
        self.model_name = model_name or config["model"]["name"]

    def generate(self, prompt: str) -> str:
        try:
            llm = get_llm()
            response = llm.invoke(prompt)
            if hasattr(response, "content"):
                return str(response.content)
            return str(response)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ OpenRouterAdapter: {e}")
            return ""

    def get_model_name(self) -> str:
        return self.model_name


def create_deepeval_metrics(model_name: str | None = None, temperature: float = 0.0):
    """–°–æ–∑–¥–∞–µ—Ç DeepEval –º–µ—Ç—Ä–∏–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–æ–≤–æ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞"""
    if not DEEPEVAL_AVAILABLE:
        print("‚ö†Ô∏è DeepEval –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        return []

    model = create_deepeval_adapter(model_name, temperature)
    if model is None:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å DeepEval –∞–¥–∞–ø—Ç–µ—Ä")
        return []

    metrics = [
        FaithfulnessMetric(threshold=0.7, model=model, include_reason=True),
        AnswerRelevancyMetric(threshold=0.7, model=model, include_reason=True),
        ContextualRelevancyMetric(threshold=0.7, model=model, include_reason=True),
    ]
    return metrics


def create_correctness_metric(model_name: str | None = None, temperature: float = 0.0):
    """–°–æ–∑–¥–∞–µ—Ç –º–µ—Ç—Ä–∏–∫—É Correctness —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–æ–≤–æ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞"""
    if not DEEPEVAL_AVAILABLE:
        return None

    model = create_deepeval_adapter(model_name, temperature)
    if model is None:
        return None

    return GEval(
        name="Correctness",
        criteria="Determine whether the actual output is factually correct based on the expected output.",
        evaluation_steps=[
            "Check whether the facts in 'actual output' contradict any facts in 'expected output'",
            "You should also heavily penalize omission of detail",
            "Vague language, or contradicting OPINIONS, are OK",
        ],
        evaluation_params=["actual output", "expected output"],
        model=model,
    )


def evaluate_rag_triad_single(
    question: str,
    answer: str,
    contexts: list[str],
    model_name: str | None = None,
    temperature: float = 0.0,
) -> dict[str, Any]:
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ–¥–∏–Ω–æ—á–Ω—ã–π –ø—Ä–∏–º–µ—Ä –ø–æ RAG Triad –º–µ—Ç—Ä–∏–∫–∞–º"""
    if not DEEPEVAL_AVAILABLE:
        return {"error": "DeepEval –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω"}

    try:
        metrics = create_deepeval_metrics(model_name, temperature)
        if not metrics:
            return {"error": "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏"}

        test_case = LLMTestCase(input=question, actual_output=answer, retrieval_context=contexts)

        results = {}
        for metric in metrics:
            try:
                metric.measure(test_case)
                results[metric.__class__.__name__] = {
                    "score": metric.score,
                    "success": metric.success,
                    "reason": getattr(metric, "reason", None),
                }
            except Exception as e:
                results[metric.__class__.__name__] = {"error": str(e)}

        return results

    except Exception as e:
        return {"error": f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ RAG Triad: {e}"}


def evaluate_correctness_single(
    question: str,
    answer: str,
    expected_answer: str,
    model_name: str | None = None,
    temperature: float = 0.0,
) -> dict[str, Any]:
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ–¥–∏–Ω–æ—á–Ω—ã–π –ø—Ä–∏–º–µ—Ä –ø–æ –º–µ—Ç—Ä–∏–∫–µ Correctness"""
    if not DEEPEVAL_AVAILABLE:
        return {"error": "DeepEval –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω"}

    try:
        metric = create_correctness_metric(model_name, temperature)
        if not metric:
            return {"error": "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –º–µ—Ç—Ä–∏–∫—É"}

        test_case = LLMTestCase(
            input=question, actual_output=answer, expected_output=expected_answer
        )

        metric.measure(test_case)
        return {
            "Correctness": {
                "score": metric.score,
                "success": metric.success,
                "reason": getattr(metric, "reason", None),
            }
        }

    except Exception as e:
        return {"error": f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ Correctness: {e}"}


def get_doc_content(doc) -> str:
    if hasattr(doc, "page_content"):
        content = doc.page_content
        if isinstance(content, list | dict):
            return str(content)
        return str(content)
    elif isinstance(doc, dict):
        content = doc.get("page_content") or doc.get("content") or doc.get("text")
        if isinstance(content, list | dict):
            return str(content)
        return str(content) if content else ""
    return str(doc)


def load_bleurt_model(model_name: str = "lucadiliello/BLEURT-20"):
    if not BLEURT_AVAILABLE:
        return None, None

    try:
        config_bleurt = BleurtConfig.from_pretrained(model_name)
        model = BleurtForSequenceClassification.from_pretrained(model_name, config=config_bleurt)
        tokenizer = BleurtTokenizer.from_pretrained(model_name)
        return model, tokenizer
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ BLEURT: {e}")
        return None, None


def calculate_bleurt_score(
    references: list[str], candidates: list[str], model_name: str = "lucadiliello/BLEURT-20"
) -> list[float]:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç BLEURT –æ—Ü–µ–Ω–∫—É —Å batch processing –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    if not BLEURT_AVAILABLE:
        print("‚ö†Ô∏è BLEURT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤—ã–µ –æ—Ü–µ–Ω–∫–∏")
        return [0.0] * len(candidates)

    model, tokenizer = load_bleurt_model(model_name)
    if model is None or tokenizer is None:
        return [0.0] * len(candidates)

    try:
        with gpu_memory_manager():
            with torch.no_grad():
                # Batch processing –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                batch_size = 8
                all_scores = []

                for i in range(0, len(references), batch_size):
                    batch_refs = references[i : i + batch_size]
                    batch_cands = candidates[i : i + batch_size]

                    inputs = tokenizer(
                        batch_refs,
                        batch_cands,
                        padding="longest",
                        return_tensors="pt",
                        truncation=True,
                        max_length=512,
                    )

                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º device –º–æ–¥–µ–ª–∏
                    device = next(model.parameters()).device
                    inputs = {k: v.to(device) for k, v in inputs.items()}

                    batch_scores = model(**inputs).logits.flatten().cpu().tolist()
                    all_scores.extend(batch_scores)

                    # –û—á–∏—â–∞–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—É—é –ø–∞–º—è—Ç—å
                    del inputs
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

                return all_scores
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è BLEURT: {e}")
        return [0.0] * len(candidates)


def calculate_cosine_similarity(reference: str, candidate: str) -> float:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–µ–∫—Å—Ç–∞–º–∏"""
    try:
        from sentence_transformers import SentenceTransformer

        model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        embeddings = model.encode([reference, candidate])
        return float(cosine_similarity([embeddings[0]], [embeddings[1]])[0][0])
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞: {e}")
        return 0.0


def calculate_cosine_similarity_batch(texts1: list[str], texts2: list[str]) -> list[float]:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è –Ω–∞–±–æ—Ä–∞–º–∏ —Ç–µ–∫—Å—Ç–æ–≤ (batch –≤–µ—Ä—Å–∏—è)"""
    try:
        from sentence_transformers import SentenceTransformer

        model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –æ–±–æ–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤
        embeddings1 = model.encode(texts1)
        embeddings2 = model.encode(texts2)

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å –º–µ–∂–¥—É —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
        similarities = [
            float(cosine_similarity([emb1], [emb2])[0][0])
            for emb1, emb2 in zip(embeddings1, embeddings2, strict=False)
        ]

        return similarities
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ (batch): {e}")
        return [0.0] * len(texts1)


def evaluate_batch(
    eval_data: list[dict[str, Any]],
    metrics: list[str] | None = None,
    model_name: str | None = None,
    temperature: float = 0.0,
) -> list[dict[str, Any]]:
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –±–∞—Ç—á –¥–∞–Ω–Ω—ã—Ö –ø–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º"""
    if metrics is None:
        metrics = ["rag_triad", "bleurt", "cosine_similarity"]

    results = []

    for i, item in enumerate(eval_data):
        print(f"–û—Ü–µ–Ω–∫–∞ {i + 1}/{len(eval_data)}")

        result = {"question": item["question"], "answer": item["answer"]}

        contexts = item.get("contexts", [])
        if isinstance(contexts, str):
            contexts = [contexts]

        expected_answer = item.get("expected_answer", "")

        # RAG Triad
        if "rag_triad" in metrics:
            rag_results = evaluate_rag_triad_single(
                item["question"], item["answer"], contexts, model_name, temperature
            )
            if "error" in rag_results:
                result["rag_triad_error"] = rag_results.get("error")
            else:
                for metric_name, payload in rag_results.items():
                    if not isinstance(payload, dict):
                        continue
                    score = payload.get("score")
                    if isinstance(score, int | float):
                        clean_name = metric_name.replace("Metric", "")
                        result[f"{clean_name}_score"] = float(score)

        # Correctness
        if "correctness" in metrics and expected_answer:
            correctness_results = evaluate_correctness_single(
                item["question"], item["answer"], expected_answer, model_name, temperature
            )
            if "error" in correctness_results:
                result["correctness_error"] = correctness_results.get("error")
            else:
                for metric_name, payload in correctness_results.items():
                    if not isinstance(payload, dict):
                        continue
                    score = payload.get("score")
                    if isinstance(score, int | float):
                        result[f"{metric_name}_score"] = float(score)

        # BLEURT
        if "bleurt" in metrics and expected_answer:
            bleurt_scores = calculate_bleurt_score([expected_answer], [item["answer"]])
            result["bleurt_score"] = bleurt_scores[0] if bleurt_scores else 0.0

        # Cosine Similarity
        if "cosine_similarity" in metrics and expected_answer:
            cosine_score = calculate_cosine_similarity(expected_answer, item["answer"])
            result["cosine_similarity"] = cosine_score

        results.append(result)

    return results


def save_evaluation_results(results: list[dict[str, Any]], output_file: str):
    df = pd.DataFrame(results)
    df.to_csv(output_file, index=False)
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")


def aggregate_results(results: list[dict[str, Any]]) -> dict[str, float]:
    if not results:
        return {}

    aggregated = {}
    numeric_columns = []

    # –ù–∞–π–¥–µ–º –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    for result in results:
        for key, value in result.items():
            if isinstance(value, int | float) and key not in ["question", "answer"]:
                if key not in numeric_columns:
                    numeric_columns.append(key)

    # –í—ã—á–∏—Å–ª–∏–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    for col in numeric_columns:
        values = [result[col] for result in results if isinstance(result.get(col), int | float)]
        if values:
            aggregated[f"avg_{col}"] = sum(values) / len(values)

    return aggregated
