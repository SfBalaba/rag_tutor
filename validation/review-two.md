1. Описание Архитектуры в readme
* Добавлен раздел про валидацию в `evals/README.md`: один основной путь (BLEURT/DeepEval), форматы CSV, подключение агента, пути к данным.
* Конфиг: `evals/research/config.yaml` — пути к чанкам/FAISS, параметры генерации/метрик, модель эмбеддингов.
* Зависимости: указаны в `pyproject.toml` + явные доп. пакеты (`deepeval`, `bleurt-pytorch`, `openai`, `sentence-transformers`) для полного пайплайна.

2. Реализация основной логики (валидация)
* Полный пайплайн: `evals/research/` — скрипты `generate_groundtruth.py` (синтетика из вектора/LLM), `evaluate.py` (замер BLEURT/DeepEval + cosine), `evaluate_models.py`/`evaluate_retrieval.py`, готовый README и config.
* Датасеты: `evals/golden_sets/math_golden_sample.csv` (университет, 3 примера) + слот для синтетики `evals/research/data/synthetic_qa.csv`.

3. Первичное тестирование (валидация)
* Формат CSV проверен (4 колонки, пути с запятыми читаются корректно).
* To-do после интеграции агента: прогнать `evals/research/scripts/evaluate.py` на его ответах, расширить golden set (школа/универ), подключить `core.*` и heavy метрики.

4. Отчёт (валидация)
* Описаны архитектура и взаимодействие компонентов в `evals/README.md`.
* Приведены пути/форматы тестовой выборки и пайплайна тестирования; метрики и команды запуска задокументированы.
* Метрики полного пайплайна будут сняты после подключения агента (коллега добавит результаты).
