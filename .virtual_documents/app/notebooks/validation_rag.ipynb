from dotenv import load_dotenv
import os
import sys
from pathlib import Path
from contextlib import asynccontextmanager
# Load environment variables from .env file
load_dotenv()

# Configure LangSmith tracing
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = os.getenv("LANGCHAIN_PROJECT", "rag-tutor")

# Initialize LangSmith client (optional, but ensures connection)
try:
    from langsmith import Client
    langsmith_api_key = os.getenv("LANGCHAIN_API_KEY")
    if langsmith_api_key:
        langsmith_client = Client(api_key=langsmith_api_key)
        print("‚úÖ LangSmith tracing enabled")
    else:
        print("‚ö†Ô∏è LANGCHAIN_API_KEY not set, tracing may not work")
except ImportError:
    print("‚ö†Ô∏è langsmith package not installed, tracing may not work")
except Exception as e:
    print(f"‚ö†Ô∏è LangSmith initialization warning: {e}")

# Add parent directory to path for imports
backend_dir = Path('/home/sofya/Documents/github/rag_tutor/app/backend').parent
app_dir = backend_dir.parent
sys.path.insert(0, str(Path('/home/sofya/Documents/github/rag_tutor/app/backend').parent))

from backend.agents.math_tutor_agent import MathTutorAgent



global agent
# Startup
try:
    # Determine paths - check if we're in Docker first
    if os.path.exists("/app"):
        # In Docker
        faiss_db_path = "/app/data/faiss_db"
        chunks_meta_path = "/app/data/all_chunks_with_meta_all.pickle"
    else:
        # Not in Docker, use relative paths
        project_root = Path('/home/sofya/Documents/github/rag_tutor')
        faiss_db_path = str(project_root / "data" / "faiss_db")
        chunks_meta_path = str(project_root / "data" / "all_chunks_with_meta_all.pickle")

    agent = MathTutorAgent(
        use_rag=os.getenv("USE_RAG", "true").lower() == "true",
        faiss_db_path=faiss_db_path,
        chunks_meta_path=chunks_meta_path
    )
    print("‚úÖ Math Tutor Agent initialized successfully")
except Exception as e:
    print(f"‚ùå Failed to initialize agent: {e}")
    raise

agent = None
print("üëã Math Tutor Agent shut down")

# Global agent instance
agent: MathTutorAgent = None

agent = MathTutorAgent(
            use_rag=os.getenv("USE_RAG", "true").lower() == "true",
            faiss_db_path=faiss_db_path,
            chunks_meta_path=chunks_meta_path
        )



import backend
from  importlib import reload
reload(backend)
from backend.agents.math_tutor_agent import MathTutorAgent


agent = MathTutorAgent(
            use_rag=os.getenv("USE_RAG", "true").lower() == "true",
            faiss_db_path=faiss_db_path,
            chunks_meta_path=chunks_meta_path
        )


import json
with open('validation_all.json', 'r', encoding='utf-8') as f:     
    validation_data_all = json.loads(f.read())


validation_data_all[0]


question = validation_data_all[0]
qq = question['query']
response = agent.chat(
            message=qq,
            conversation_history=''
        )
response


response['response']


question['reference_answer']


import pickle
chunks_meta_path = '/home/sofya/Documents/github/rag_tutor/data/all_chunks_with_meta_all.pickle'
chunks = pickle.load(open(chunks_meta_path, 'rb'))
chunks[0]


len(chunks)


retriver = agent.rag_retriever
chunks = retriver.retrieve('–∫–∞–∫ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ —á–∞—Å—Ç—è–º')
chunks


chunks[0]['metadata'].keys()


retriever = retriver
val_set = validation_data_all.copy()
results_all =  []
results_val_rag = []
for k in [5, 10, 20, 30, 50]:
    metrics = {"recall": 0, "section_hit": 0, "level_consistency": 0, "mrr": 0, "accuracy@1": 0, 'precision': 0, "hit": 0}
    for i, item in enumerate(val_set):
        relevant = set(zip(item["relevant_chunk_ids"], [item['book_title']] * len(item["relevant_chunk_ids"])))
        
        results = retriever.retrieve(item["query"], initial_retrieval_k=k, top_k=5)
        retrieved_ids = {(r['metadata']["chunk_id"] , r['metadata']["book_title"]) for r in results}

        retrieved_sections = {r['metadata'].get("source_file") for r in results}
        
        relevant_retrieved = len(relevant & retrieved_ids) 
        # Recall@K
        recall = relevant_retrieved / len(relevant) if len(relevant) != 0 else 0
        metrics["recall"] += recall
        
        # Section Hit
        section_hit = int(bool(retrieved_ids & relevant))
        metrics["section_hit"] += section_hit

        
        # Level Consistency
        level_ok = sum(1 for r in results if r['metadata']["level"] == item["level"])
        level_consistency = level_ok / len(results) if len(results) else  0
        metrics["level_consistency"] += level_consistency


        precision = relevant_retrieved / len(retrieved_ids) if retrieved_ids else 0
        recall = relevant_retrieved  / len(relevant)
        hit = 1.0 if relevant_retrieved else 0.0

        metrics['precision'] += precision
        metrics['hit'] += hit


        # MRR
        mrr = 0
        for rank, r in enumerate(results, 1):
            if r['metadata']["chunk_id"] in relevant:
                mrr = 1 / rank
                metrics["mrr"] += mrr
                break

        results_all.append({"id": i,
         "recall": recall,
         "section_hit": section_hit,
         "level_consistency": level_consistency,
         "mrr": mrr , 
         'precision': precision, 
         "hit": hit,
         "k": k}
                      )
    n = len(val_set)
    dict_results = {k: v/n for k, v in metrics.items()}
    dict_results['k'] = k
    results_val_rag.append(dict_results)


pd.DataFrame(results_all).groupby('k').agg('mean')


import pandas as pd
pd.DataFrame(results_val_rag)


from pydantic import BaseModel, Field
from typing import Literal
import asyncio
from collections import Counter
from langchain_mistralai import ChatMistralAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import os
from typing import List
import json
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from pathlib import Path
from tqdm.notebook import tqdm

MISTRAL_API_KEY = 'jo4Ujl29HvTWtD0B4JWlfv5yndiGXfdX'
# –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—é–±—É—é –º–æ–¥–µ–ª—å –æ—Ç –ª—é–±–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞, Mistral —Ç—É—Ç –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
chat = ChatMistralAI(
    api_key=MISTRAL_API_KEY,
    model_name='mistral-large-2407'
)


# –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è structured output
class JudgeResponse(BaseModel):
    score: Literal["1.0", "0.5", "0.0", "-1.0"] = Field(
        description="Truthfulness score: 1.0=correct, 0.5=partial, 0.0=refusal, -1.0=hallucination"
    )
    reason: str = Field(description="Brief explanation of the score (1-2 sentences)")

# Judge prompt –¥–ª—è –æ—Ü–µ–Ω–∫–∏ truthfulness
JUDGE_SYSTEM_PROMPT = """You are an impartial judge evaluating the truthfulness of an AI assistant's answer.

You will be given:
1. A question
2. The correct answer(s) 
3. The AI assistant's response

Score the response using this scale:
- 1.0: Completely correct answer that matches the expected answer
- 0.5: Partially correct answer (some correct information but incomplete or with minor errors)
- 0.0: Refusal to answer (e.g., "I don't know", "I cannot answer", "no information in context")
- -1.0: Hallucination - the AI gave a CONFIDENT FACTUAL ANSWER that is WRONG

CRITICAL DISTINCTION between 0.0 and -1.0:
- Score 0.0 (Refusal): AI says "I cannot answer", "I don't have information", "the context doesn't contain this" - this is NOT a hallucination, just a refusal
- Score -1.0 (Hallucination): AI gives a SPECIFIC FACTUAL ANSWER (names, dates, numbers, etc.) that is INCORRECT

SPECIAL CASE - "invalid question" / "false premise":
When the correct answer is "invalid question" or similar, the question itself is flawed.
- Score 1.0: AI identifies the false premise OR refuses to answer
- Score -1.0: AI gives a confident factual answer (hallucination)

IMPORTANT:
- A refusal is ALWAYS score 0.0 (or 1.0 in SPECIAL CASE above), never -1.0, regardless of whether correct answer exists
- Only give -1.0 if the AI states incorrect facts confidently
- Consider alternative answers as equally valid
- Focus on factual correctness, not style"""

JUDGE_USER_PROMPT = """Question: {question}

Correct Answer: {correct_answer}

AI Response: {response}

Evaluate the response: 
- If AI refused to answer ‚Üí Score 0.0
- If AI gave wrong facts confidently ‚Üí Score -1.0
- If AI answered correctly ‚Üí Score 1.0 or 0.5"""

judge_prompt = ChatPromptTemplate.from_messages([
    ("system", JUDGE_SYSTEM_PROMPT),
    ("human", JUDGE_USER_PROMPT)
])

# –ò—Å–ø–æ–ª—å–∑—É–µ–º structured output
judge_chain = judge_prompt | chat.with_structured_output(JudgeResponse)


async def get_rag_response_async(question_data: dict, k: int = 10):
    question = question_data['query']

    response = agent.chat(message=question)

    return response['response']


async def judge_response_async(question_data: dict, response: str):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –æ—Ç–≤–µ—Ç–∞ —Å structured output."""
    alt_answers = question_data.get('alt_ans', [])

    try:
        result: JudgeResponse = await judge_chain.ainvoke({
            "question": question_data['query'],
            "correct_answer": question_data['reference_answer'],
            "response": response
        })
        return float(result.score), result.reason
    except Exception as e:
        return 0.0, f"Error: {str(e)}"




async def evaluate_all_questions(questions_list: list, k: int = 10):

    # –®–∞–≥ 1: –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –æ—Ç–≤–µ—Ç—ã RAG
    responses = []
    try:
        for q in questions_list:
            response = await get_rag_response_async(q, k)
            responses.append(response)
    except Exception as e:
        print(e)
        return responses
    # –ï—Å–ª–∏ –Ω–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ 1 RPS –∫–∞–∫ —É Mistral, –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã –ø–∞—Ä–µ–ª–ª–µ–ª—å–Ω–æ

    # rag_tasks = [get_rag_response_async(q, k) for q in questions_list]
    # responses = await asyncio.gather(*rag_tasks)

    # –ò–∑–±–µ–≥–∞–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º–∏–Ω—É—Ç—É –æ—Ç Mistral API
    print("–ñ–¥–µ–º 60 —Å–µ–∫—É–Ω–¥ –¥–ª—è —Å–±—Ä–æ—Å–∞ –ª–∏–º–∏—Ç–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º Mistral API...")
    await asyncio.sleep(60)

    # –®–∞–≥ 2: –û—Ü–µ–Ω–∏–≤–∞–µ–º –≤—Å–µ –æ—Ç–≤–µ—Ç—ã
    evaluations = []
    for q, r in zip(questions_list, responses):
        eval = await judge_response_async(q, r)
        evaluations.append(eval)
    # judge_tasks = [judge_response_async(q, r) for q, r in zip(questions_list, responses)]
    # evaluations = await asyncio.gather(*judge_tasks)

    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = []
    for q, response, (score, reason) in zip(questions_list, responses, evaluations):
        results.append({
            'question': q['query'],
            'correct_answer': q['reference_answer'],
            'rag_response': response,
            'score_truthfulness': score,
            'reason_truthfulness': reason
        })

    return results




eval_gen = await evaluate_all_questions(validation_data_all, k=5)
eval_gen


truthfulness = pd.DataFrame(eval_gen)
truthfulness.to_csv('metrics_truthfulness.csv')


eval_data = pd.read_csv('metrics_truthfulness.csv')
eval_data.head()


truthfulness['score_truthfulness'].mean()


truthfulness[truthfulness['score_truthfulness'] != 1].reason_truthfulness.values


context = retriever.format_chunks_for_context(chunks)
agent.chat(
            message=message,
            conversation_history='',
            context=context
        )


doc = retriever.retrieve('–∫–∞–∫ —Ä–µ—à–∏—Ç—å –∫–≤–∞–¥—Ä–∞—Ç–Ω–æ–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ?')


from ragas import evaluate

from ragas.metrics.collections import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from datasets import Dataset
from langchain_core.documents import Document
import os

input_eval_data = {}
def run_ragas_evaluation(val_data, retriever, agent, k=10):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã, –∑–∞—Ç–µ–º –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏—Ö —Å –ø–æ–º–æ—â—å—é RAGAS.
    
    Args:
        val_data: —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–∏–¥–∞ {"query": "...", ...}
        retriever: –≤–∞—à —Ä–µ—Ç—Ä–∏–≤–µ—Ä —Å –º–µ—Ç–æ–¥–æ–º retrieve(query, ..., top_k=k, levels=[level])
        generator_chain: LangChain —Ü–µ–ø–æ—á–∫–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç {"context": str, "question": str} -> –æ—Ç–≤–µ—Ç (str)
        k: top_k –¥–ª—è —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
        level: —É—Ä–æ–≤–µ–Ω—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    """
    questions = []
    answers = []
    contexts_list = []  # list of list of str
    references=  []

    for item in tqdm(val_data, desc="Generating RAG responses for RAGAS"):
        # 1. –†–µ—Ç—Ä–∏–≤ —á–∞–Ω–∫–æ–≤
        retrieved = retriever.retrieve(
            query=item["query"],
            initial_retrieval_k=k,
            top_k=k,
        )
        # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        contexts = [r["text"] for r in retrieved]

        context = retriever.format_chunks_for_context(retrieved)

        answer = eval_data.query(f'question == "{item["query"]}"')['rag_response'].values[0]
        # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        questions.append(item["query"])
        answers.append(answer)
        contexts_list.append(contexts)  # RAGAS –æ–∂–∏–¥–∞–µ—Ç List[List[str]]
        references.append(item['reference_answer'])
    
    
    dataset = Dataset.from_dict({
        "question": questions,
        "answer": answers,
        "contexts": contexts_list,
        "reference": references
    })
    
    result = evaluate(
        dataset=dataset,
        metrics=[
            Faithfulness(),
            AnswerRelevancy(),
            ContextPrecision(),
            ContextRecall(),
        ],
        llm=chat,  
        embeddings=HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")  # –¥–ª—è context_precision/recall
    )
    
    return result


result_ragas = run_ragas_evaluation(validation_data_all, retriever, agent, k=5)


pickle.dump(result_ragas, open('ragas_results.pickle', 'wb'))


result_ragas.to_pandas()


with open('regas_metrics.txt', 'w') as f:
    f.write(str(result_ragas))


result_ragas.to_pandas().to_csv('result_ragas_df.csv')


all_metrics = result_ragas._repr_dict.copy()
truthfulness_val = eval_data['score_truthfulness'].mean()
all_metrics['truthfulness'] = truthfulness_val
all_metrics



