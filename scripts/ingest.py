# scripts/ingest.py
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import re
import json
import shutil
import tempfile
import subprocess
from pathlib import Path
from typing import List
from langchain_core.documents import Document
from langchain_community.document_loaders import (
    PyMuPDFLoader,
    UnstructuredWordDocumentLoader,
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
import chromadb
from tqdm import tqdm


DATA_DIR = Path("../data")
VECTOR_DB_PATH = Path("../vectorbase")
SAMPLES_PATH = Path("../samples/sample_chunks.json")

LEVELS = ["elementary", "middle_school", "high_school", "university"]

CHUNK_PARAMS = {
    "elementary": {"chunk_size": 300, "chunk_overlap": 50},
    "middle_school": {"chunk_size": 400, "chunk_overlap": 60},
    "high_school": {"chunk_size": 500, "chunk_overlap": 80},
    "university": {"chunk_size": 700, "chunk_overlap": 100},
}

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
embedding_model = SentenceTransformer("/home/sofya/all-MiniLM-L6-v2")



def extract_grade_from_path(file_path: Path, level_dir: Path) -> str:
    try:
        rel_parts = file_path.relative_to(level_dir).parts
        for part in rel_parts:
            if "–∫–ª–∞—Å—Å" in part or "course" in part.lower():
                match = re.search(r'(\d+)', part)
                if match:
                    return match.group(1)
    except ValueError:
        pass
    return "general"

def get_all_document_files(base_dir: Path) -> List[Path]:
    supported_ext = {".pdf", ".doc", ".docx", ".djvu"}
    files = []
    for file_path in base_dir.rglob("*"):
        if file_path.is_file() and file_path.suffix.lower() in supported_ext:
            if file_path.name.startswith("."):
                continue
            files.append(file_path)
    return files


try:
    from pdf2image import convert_from_path
    import pytesseract
    TESSERACT_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è OCR: pip install pdf2image pytesseract")
    TESSERACT_AVAILABLE = False

def ocr_pdf_to_text(pdf_path: Path, lang: str = "rus") -> List[str]:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç PDF –≤ —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ OCR (–æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É)."""
    if not TESSERACT_AVAILABLE:
        return []
    try:
        images = convert_from_path(str(pdf_path), dpi=200)
        texts = []
        for img in images:
            text = pytesseract.image_to_string(img, lang=lang)
            texts.append(text)
        return texts
    except Exception as e:
        print(f"  ‚ùå OCR –æ—à–∏–±–∫–∞: {e}")
        return []

def load_document(file_path: Path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π OCR –¥–ª—è —Å–∫–∞–Ω–æ–≤."""
    ext = file_path.suffix.lower()

    if ext == ".pdf":
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é
        docs = PyMuPDFLoader(str(file_path)).load()
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π ‚Äî –ø—Ä–∏–º–µ–Ω—è–µ–º OCR
        if not any(doc.page_content.strip() for doc in docs):
            print(f"  ‚ö†Ô∏è PDF {file_path.name} ‚Äî –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç. –ü—Ä–∏–º–µ–Ω—è—é OCR...")
            ocr_texts = ocr_pdf_to_text(file_path)
            docs = [
                Document(page_content=text, metadata={"source": str(file_path), "page": i})
                for i, text in enumerate(ocr_texts) if text.strip()
            ]
        return docs

    elif ext in (".doc", ".docx"):
        return UnstructuredWordDocumentLoader(str(file_path)).load()

    elif ext == ".djvu":
        if not shutil.which("ddjvu"):
            print("  ‚ö†Ô∏è –£—Ç–∏–ª–∏—Ç–∞ 'ddjvu' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–∞–∫–µ—Ç 'djvulibre'.")
            return []
        try:
            with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp:
                tmp_pdf = Path(tmp.name)
            result = subprocess.run(
                ["ddjvu", "-format=pdf", str(file_path), str(tmp_pdf)],
                capture_output=True,
                text=True,
                timeout=120
            )
            if result.returncode != 0:
                print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ ddjvu: {result.stderr}")
                tmp_pdf.unlink(missing_ok=True)
                return []
            # –ü—Ä–∏–º–µ–Ω—è–µ–º OCR –∫ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É PDF
            print(f"  ‚ö†Ô∏è —Ä–ü—Ä–∏–º–µ–Ω—è–µ–º OCR –∫ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É PDF {tmp_pdf.name}. –ü—Ä–∏–º–µ–Ω—è—é OCR...")
            
            ocr_texts = ocr_pdf_to_text(tmp_pdf)
            tmp_pdf.unlink(missing_ok=True)
            docs = [
                Document(page_content=text, metadata={"source": str(file_path), "page": i})
                for i, text in enumerate(ocr_texts) if text.strip()
            ]
            return docs
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ DJVU {file_path.name}: {e}")
            tmp_pdf.unlink(missing_ok=True)
            return []

    else:
        return []



def main():
    client = chromadb.PersistentClient(path=str(VECTOR_DB_PATH))
    all_sample_chunks = []

    for level in LEVELS:
        level_dir = DATA_DIR / level
        if not level_dir.exists():
            print(f"‚ö†Ô∏è –£—Ä–æ–≤–µ–Ω—å '{level}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            continue

        print(f"\nüìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞ —É—Ä–æ–≤–Ω—è: {level}")
        file_paths = get_all_document_files(level_dir)
        print(f"  –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(file_paths)}")

        if not file_paths:
            continue

        params = CHUNK_PARAMS[level]
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=params["chunk_size"],
            chunk_overlap=params["chunk_overlap"],
            separators=["\n\n", "\n", ". ", " ", ""],
        )

        all_texts, all_metadatas, all_ids = [], [], []

        for file_path in tqdm(file_paths, desc=f"  –ó–∞–≥—Ä—É–∑–∫–∞ ({level})"):
            if "checkpoint" in file_path.name:
                continue
            try:
                documents = load_document(file_path)
                if not documents:
                    continue

                chunks = text_splitter.split_documents(documents)
                grade = extract_grade_from_path(file_path, level_dir)
                source_rel = str(file_path.relative_to(DATA_DIR))

                for i, chunk in enumerate(chunks):
                    text = chunk.page_content.strip()
                    if not text:
                        continue
                    meta = {
                        "level": level,
                        "grade": grade,
                        "source": source_rel,
                        "filename": file_path.name,
                    }
                    if hasattr(chunk, "metadata") and isinstance(chunk.metadata, dict):
                        meta.update({
                            k: v for k, v in chunk.metadata.items()
                            if isinstance(v, (str, int, float, bool)) and k not in meta
                        })
                    chunk_id = f"{level}_{file_path.stem}_{i}"
                    all_texts.append(text)
                    all_metadatas.append(meta)
                    all_ids.append(chunk_id)

                    if len(all_sample_chunks) < 10:
                        all_sample_chunks.append({
                            "id": chunk_id,
                            "text": text[:200] + "..." if len(text) > 200 else text,
                            "metadata": meta
                        })

            except Exception as e:
                print(f"\n  ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path}: {e}")
                continue

        if not all_texts:
            print(f"  ‚ö†Ô∏è –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è —É—Ä–æ–≤–Ω—è '{level}'")
            continue

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Chroma (–æ–¥–∏–Ω —Ä–∞–∑!)
        collection = client.get_or_create_collection(name=level, embedding_function=None)
        embeddings = embedding_model.encode(all_texts, convert_to_numpy=True).tolist()
        collection.add(
            documents=all_texts,
            metadatas=all_metadatas,
            embeddings=embeddings,
            ids=all_ids
        )
        print(f"  ‚úÖ –£—Ä–æ–≤–µ–Ω—å '{level}' —Å–æ—Ö—Ä–∞–Ω—ë–Ω ({len(all_texts)} —á–∞–Ω–∫–æ–≤).")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ç–º–ø–ª—ã
    SAMPLES_PATH.parent.mkdir(parents=True, exist_ok=True)
    with open(SAMPLES_PATH, "w", encoding="utf-8") as f:
        json.dump(all_sample_chunks, f, indent=2, ensure_ascii=False)

    print(f"\nüéâ –ò–Ω–≥–µ—Å—Ç –∑–∞–≤–µ—Ä—à—ë–Ω. –°—ç–º–ø–ª—ã: {SAMPLES_PATH}")

if __name__ == "__main__":
    main()