{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6927d2a3-844f-4b5a-b644-0f3315297be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scripts/ingest.py\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from langchain_community.document_loaders import (\n",
    "    PyMuPDFLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===\n",
    "DATA_DIR = Path(\"../data\")\n",
    "VECTOR_DB_PATH = Path(\"../vectorstore\")\n",
    "SAMPLES_PATH = Path(\"../samples/sample_chunks.json\")\n",
    "\n",
    "# –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —É—Ä–æ–≤–Ω–∏ (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–∞–ø–∫–∞–º–∏ –≤ DATA_DIR)\n",
    "LEVELS = [\"elementary\", \"middle_school\", \"high_school\", \"university\"]\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —á–∞–Ω–∫–∏–Ω–≥–∞ –ø–æ —É—Ä–æ–≤–Ω—é\n",
    "CHUNK_PARAMS = {\n",
    "    \"elementary\": {\"chunk_size\": 300, \"chunk_overlap\": 50},\n",
    "    \"middle_school\": {\"chunk_size\": 400, \"chunk_overlap\": 60},\n",
    "    \"high_school\": {\"chunk_size\": 500, \"chunk_overlap\": 80},\n",
    "    \"university\": {\"chunk_size\": 700, \"chunk_overlap\": 100},\n",
    "}\n",
    "\n",
    "# –≠–º–±–µ–¥–¥–∏–Ω–≥-—Ñ—É–Ω–∫—Ü–∏—è (–ª–æ–∫–∞–ª—å–Ω–∞—è, –±–µ–∑ API)\n",
    "# embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "#     model_name=\"all-MiniLM-L6-v2\"\n",
    "# )\n",
    "embedding_model = SentenceTransformer(\"/home/sofya/all-MiniLM-L6-v2\")\n",
    "\n",
    "def extract_grade_from_path(file_path: Path, level_dir: Path) -> str:\n",
    "    \"\"\"\n",
    "    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä –∫–ª–∞—Å—Å–∞ –∏–∑ –ø—É—Ç–∏, –µ—Å–ª–∏ –µ—Å—Ç—å –ø–æ–¥–ø–∞–ø–∫–∞ –≤–∏–¥–∞ '5 –∫–ª–∞—Å—Å', '10 –∫–ª–∞—Å—Å' –∏ —Ç.–¥.\n",
    "    –ò—â–µ—Ç —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ level_dir.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        rel_parts = file_path.relative_to(level_dir).parts\n",
    "        for part in rel_parts:\n",
    "            if \"–∫–ª–∞—Å—Å\" in part or 'course' in part:\n",
    "                match = re.search(r'(\\d+)', part)\n",
    "                if match:\n",
    "                    return match.group(1)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return \"general\"  # –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
    "\n",
    "def get_all_document_files(base_dir: Path) -> List[Path]:\n",
    "    \"\"\"–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ.\"\"\"\n",
    "    supported_ext = {\".pdf\", \".doc\", \".docx\", \".djvu\"}\n",
    "    files = []\n",
    "    for file_path in base_dir.rglob(\"*\"):\n",
    "        if file_path.is_file() and file_path.suffix.lower() in supported_ext:\n",
    "            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–∫—Ä—ã—Ç—ã–µ —Ñ–∞–π–ª—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, .DS_Store)\n",
    "            if file_path.name.startswith(\".\"):\n",
    "                continue\n",
    "            files.append(file_path)\n",
    "    return files\n",
    "\n",
    "\n",
    "def load_document(file_path: Path):\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç PDF, DOC(X), DJVU (—á–µ—Ä–µ–∑ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é).\"\"\"\n",
    "    ext = file_path.suffix.lower()\n",
    "    \n",
    "    if ext == \".pdf\":\n",
    "        return PyMuPDFLoader(str(file_path)).load()\n",
    "    \n",
    "    elif ext in (\".doc\", \".docx\"):\n",
    "        return UnstructuredWordDocumentLoader(str(file_path)).load()\n",
    "    \n",
    "    elif ext == \".djvu\":\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ ddjvu\n",
    "        if not shutil.which(\"ddjvu\"):\n",
    "            print(\"  ‚ö†Ô∏è –£—Ç–∏–ª–∏—Ç–∞ 'ddjvu' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–∞–∫–µ—Ç 'djvulibre'. DJVU-—Ñ–∞–π–ª –ø—Ä–æ–ø—É—â–µ–Ω.\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".pdf\", delete=False) as tmp_pdf:\n",
    "                tmp_pdf_path = Path(tmp_pdf.name)\n",
    "\n",
    "            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º DJVU ‚Üí PDF\n",
    "            result = subprocess.run(\n",
    "                [\"ddjvu\", \"-format=pdf\", str(file_path), str(tmp_pdf_path)],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=120  # –º–∞–∫—Å 2 –º–∏–Ω—É—Ç—ã –Ω–∞ —Ñ–∞–π–ª\n",
    "            )\n",
    "\n",
    "            if result.returncode != 0:\n",
    "                print(f\"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ DJVU ‚Üí PDF: {result.stderr}\")\n",
    "                tmp_pdf_path.unlink(missing_ok=True)\n",
    "                return []\n",
    "\n",
    "            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—ã–π PDF\n",
    "            docs = PyMuPDFLoader(str(tmp_pdf_path)).load()\n",
    "            tmp_pdf_path.unlink(missing_ok=True)  # —É–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª\n",
    "            return docs\n",
    "\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(f\"  ‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ DJVU: {file_path.name}\")\n",
    "            tmp_pdf_path.unlink(missing_ok=True)\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ DJVU: {e}\")\n",
    "            tmp_pdf_path.unlink(missing_ok=True)\n",
    "            return []\n",
    "    \n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def main():\n",
    "    client = chromadb.PersistentClient(path=str(VECTOR_DB_PATH))\n",
    "    all_sample_chunks = []\n",
    "\n",
    "    for level in LEVELS:\n",
    "        level_dir = DATA_DIR / level\n",
    "        if not level_dir.exists():\n",
    "            print(f\"‚ö†Ô∏è –£—Ä–æ–≤–µ–Ω—å '{level}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç (–ø–∞–ø–∫–∞ {level_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞). –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞ —É—Ä–æ–≤–Ω—è: {level}\")\n",
    "        file_paths = get_all_document_files(level_dir)\n",
    "        print(f\"  –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(file_paths)}\")\n",
    "\n",
    "        if not file_paths:\n",
    "            print(f\"  ‚ö†Ô∏è –ù–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ {level_dir}\")\n",
    "            continue\n",
    "\n",
    "        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —á–∞–Ω–∫–∏–Ω–≥–∞\n",
    "        params = CHUNK_PARAMS.get(level, CHUNK_PARAMS[\"university\"])\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=params[\"chunk_size\"],\n",
    "            chunk_overlap=params[\"chunk_overlap\"],\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "            length_function=len,\n",
    "        )\n",
    "\n",
    "        all_chunks = []\n",
    "        all_metadatas = []\n",
    "        all_texts = []\n",
    "        all_ids = []\n",
    "\n",
    "        for file_path in tqdm(file_paths, desc=f\"  –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ ({level})\"):\n",
    "            if 'checkpoint' in file_path.name:\n",
    "                continue\n",
    "            try:\n",
    "                documents = load_document(file_path)\n",
    "                if not documents:\n",
    "                    continue\n",
    "\n",
    "                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏\n",
    "                chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "                grade = extract_grade_from_path(file_path, level_dir)\n",
    "                # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–±—É–¥–µ—Ç –ø–æ–∫–∞–∑–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é)\n",
    "                source_rel_path = str(file_path.relative_to(DATA_DIR))\n",
    "\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    text = chunk.page_content.strip()\n",
    "                    if not text:\n",
    "                        continue\n",
    "\n",
    "                    metadata = {\n",
    "                        \"level\": level,\n",
    "                        \"grade\": grade,\n",
    "                        \"source\": source_rel_path,  # <-- —ç—Ç–æ –±—É–¥–µ—Ç –≤ –æ—Ç–≤–µ—Ç–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è!\n",
    "                        \"filename\": file_path.name,\n",
    "                    }\n",
    "                    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ PDF)\n",
    "                    if hasattr(chunk, 'metadata') and isinstance(chunk.metadata, dict):\n",
    "                        metadata.update({\n",
    "                            k: v for k, v in chunk.metadata.items()\n",
    "                            if isinstance(v, (str, int, float, bool)) and k not in metadata\n",
    "                        })\n",
    "\n",
    "                    chunk_id = f\"{level}_{file_path.stem}_{i}\"\n",
    "\n",
    "                    all_texts.append(text)\n",
    "                    all_metadatas.append(metadata)\n",
    "                    all_ids.append(chunk_id)\n",
    "\n",
    "                    # –°–æ–±–∏—Ä–∞–µ–º —Å—ç–º–ø–ª—ã (–Ω–µ –±–æ–ª–µ–µ 10 —á–∞–Ω–∫–æ–≤ –≤—Å–µ–≥–æ)\n",
    "                    if len(all_sample_chunks) < 10:\n",
    "                        all_sample_chunks.append({\n",
    "                            \"id\": chunk_id,\n",
    "                            \"text\": text[:200] + \"...\" if len(text) > 200 else text,\n",
    "                            \"metadata\": metadata\n",
    "                        })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\n  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"  –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è —É—Ä–æ–≤–Ω—è '{level}': {len(all_texts)}\")\n",
    "\n",
    "        if not all_texts:\n",
    "            print(f\"  ‚ö†Ô∏è –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ {level}\")\n",
    "            continue\n",
    "\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Chroma\n",
    "        # collection = client.get_or_create_collection(\n",
    "        #     name=level,\n",
    "        #     embedding_function=embedding_fn\n",
    "        # )\n",
    "        collection = client.get_or_create_collection(\n",
    "            name=level,\n",
    "            embedding_function=None  # –æ—Ç–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ-—ç–º–±–µ–¥–¥–∏–Ω–≥\n",
    "        )\n",
    "                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "        embeddings = embedding_model.encode(all_texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "        embeddings = embeddings.tolist()  # Chroma –æ–∂–∏–¥–∞–µ—Ç list[list[float]]\n",
    "        \n",
    "        collection.add(\n",
    "            documents=all_texts,\n",
    "            metadatas=all_metadatas,\n",
    "            embeddings=embeddings,\n",
    "            ids=all_ids\n",
    "        )\n",
    "        collection.add(\n",
    "            documents=all_texts,\n",
    "            metadatas=all_metadatas,\n",
    "            ids=all_ids\n",
    "        )\n",
    "\n",
    "        print(f\"  ‚úÖ –£—Ä–æ–≤–µ–Ω—å '{level}' —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ Chroma.\")\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ç–º–ø–ª—ã\n",
    "    SAMPLES_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(SAMPLES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_sample_chunks, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "    print(f\"\\nüéâ –ò–Ω–≥–µ—Å—Ç –∑–∞–≤–µ—Ä—à—ë–Ω. –°—ç–º–ø–ª—ã —á–∞–Ω–∫–æ–≤: {SAMPLES_PATH}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0452701a-e85d-4c0f-b320-ce7482585a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../vectorstore')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VECTOR_DB_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d199d6b7-c1c4-49b1-bb38-cffdd117c44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è –£—Ä–æ–≤–µ–Ω—å 'elementary' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç (–ø–∞–ø–∫–∞ ../data/elementary –Ω–µ –Ω–∞–π–¥–µ–Ω–∞). –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.\n",
      "\n",
      "üìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞ —É—Ä–æ–≤–Ω—è: middle_school\n",
      "  –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ (middle_school): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [04:06<00:00,  6.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è —É—Ä–æ–≤–Ω—è 'middle_school': 977\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Query error: Database error: error returned from database: (code: 1032) attempt to write a readonly database",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m     99\u001b[39m embeddings = embedding_model.encode(all_texts, convert_to_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, show_progress_bar=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    100\u001b[39m embeddings = embeddings.tolist()  \u001b[38;5;66;03m# Chroma –æ–∂–∏–¥–∞–µ—Ç list[list[float]]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m \u001b[43mcollection\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_metadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_ids\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m collection.add(\n\u001b[32m    109\u001b[39m     documents=all_texts,\n\u001b[32m    110\u001b[39m     metadatas=all_metadatas,\n\u001b[32m    111\u001b[39m     ids=all_ids\n\u001b[32m    112\u001b[39m )\n\u001b[32m    114\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ‚úÖ –£—Ä–æ–≤–µ–Ω—å \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ Chroma.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/last-docling/lib/python3.12/site-packages/chromadb/api/models/Collection.py:95\u001b[39m, in \u001b[36mCollection.add\u001b[39m\u001b[34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03m    ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m \n\u001b[32m     84\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     86\u001b[39m add_request = \u001b[38;5;28mself\u001b[39m._validate_and_prepare_add_request(\n\u001b[32m     87\u001b[39m     ids=ids,\n\u001b[32m     88\u001b[39m     embeddings=embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m     92\u001b[39m     uris=uris,\n\u001b[32m     93\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_add\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadatas\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43muris\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muris\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/last-docling/lib/python3.12/site-packages/chromadb/api/rust.py:441\u001b[39m, in \u001b[36mRustBindingsAPI._add\u001b[39m\u001b[34m(self, ids, collection_id, embeddings, metadatas, documents, uris, tenant, database)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_add\u001b[39m(\n\u001b[32m    421\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    429\u001b[39m     database: \u001b[38;5;28mstr\u001b[39m = DEFAULT_DATABASE,\n\u001b[32m    430\u001b[39m ) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    431\u001b[39m     \u001b[38;5;28mself\u001b[39m.product_telemetry_client.capture(\n\u001b[32m    432\u001b[39m         CollectionAddEvent(\n\u001b[32m    433\u001b[39m             collection_uuid=\u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[32m   (...)\u001b[39m\u001b[32m    438\u001b[39m         )\n\u001b[32m    439\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43muris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mInternalError\u001b[39m: Query error: Database error: error returned from database: (code: 1032) attempt to write a readonly database"
     ]
    }
   ],
   "source": [
    "client = chromadb.PersistentClient(path=str(VECTOR_DB_PATH))\n",
    "all_sample_chunks = []\n",
    "\n",
    "for level in LEVELS:\n",
    "    level_dir = DATA_DIR / level\n",
    "    if not level_dir.exists():\n",
    "        print(f\"‚ö†Ô∏è –£—Ä–æ–≤–µ–Ω—å '{level}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç (–ø–∞–ø–∫–∞ {level_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞). –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nüìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞ —É—Ä–æ–≤–Ω—è: {level}\")\n",
    "    file_paths = get_all_document_files(level_dir)\n",
    "    print(f\"  –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(file_paths)}\")\n",
    "\n",
    "    if not file_paths:\n",
    "        print(f\"  ‚ö†Ô∏è –ù–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ {level_dir}\")\n",
    "        continue\n",
    "\n",
    "    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —á–∞–Ω–∫–∏–Ω–≥–∞\n",
    "    params = CHUNK_PARAMS.get(level, CHUNK_PARAMS[\"university\"])\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=params[\"chunk_size\"],\n",
    "        chunk_overlap=params[\"chunk_overlap\"],\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "    all_metadatas = []\n",
    "    all_texts = []\n",
    "    all_ids = []\n",
    "\n",
    "    for file_path in tqdm(file_paths, desc=f\"  –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ ({level})\"):\n",
    "        if 'checkpoint' in file_path.name:\n",
    "            continue\n",
    "        try:\n",
    "            documents = load_document(file_path)\n",
    "            if not documents:\n",
    "                continue\n",
    "\n",
    "            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏\n",
    "            chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "            grade = extract_grade_from_path(file_path, level_dir)\n",
    "            # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–±—É–¥–µ—Ç –ø–æ–∫–∞–∑–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é)\n",
    "            source_rel_path = str(file_path.relative_to(DATA_DIR))\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                text = chunk.page_content.strip()\n",
    "                if not text:\n",
    "                    continue\n",
    "\n",
    "                metadata = {\n",
    "                    \"level\": level,\n",
    "                    \"grade\": grade,\n",
    "                    \"source\": source_rel_path,  # <-- —ç—Ç–æ –±—É–¥–µ—Ç –≤ –æ—Ç–≤–µ—Ç–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è!\n",
    "                    \"filename\": file_path.name,\n",
    "                }\n",
    "                # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ PDF)\n",
    "                if hasattr(chunk, 'metadata') and isinstance(chunk.metadata, dict):\n",
    "                    metadata.update({\n",
    "                        k: v for k, v in chunk.metadata.items()\n",
    "                        if isinstance(v, (str, int, float, bool)) and k not in metadata\n",
    "                    })\n",
    "\n",
    "                chunk_id = f\"{level}_{file_path.stem}_{i}\"\n",
    "\n",
    "                all_texts.append(text)\n",
    "                all_metadatas.append(metadata)\n",
    "                all_ids.append(chunk_id)\n",
    "\n",
    "                # –°–æ–±–∏—Ä–∞–µ–º —Å—ç–º–ø–ª—ã (–Ω–µ –±–æ–ª–µ–µ 10 —á–∞–Ω–∫–æ–≤ –≤—Å–µ–≥–æ)\n",
    "                if len(all_sample_chunks) < 10:\n",
    "                    all_sample_chunks.append({\n",
    "                        \"id\": chunk_id,\n",
    "                        \"text\": text[:200] + \"...\" if len(text) > 200 else text,\n",
    "                        \"metadata\": metadata\n",
    "                    })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"  –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è —É—Ä–æ–≤–Ω—è '{level}': {len(all_texts)}\")\n",
    "\n",
    "    if not all_texts:\n",
    "        print(f\"  ‚ö†Ô∏è –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ {level}\")\n",
    "        continue\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Chroma\n",
    "    # collection = client.get_or_create_collection(\n",
    "    #     name=level,\n",
    "    #     embedding_function=embedding_fn\n",
    "    # )\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=level,\n",
    "        embedding_function=None  # –æ—Ç–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ-—ç–º–±–µ–¥–¥–∏–Ω–≥\n",
    "    )\n",
    "            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "    embeddings = embedding_model.encode(all_texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "    embeddings = embeddings.tolist()  # Chroma –æ–∂–∏–¥–∞–µ—Ç list[list[float]]\n",
    "    \n",
    "    collection.add(\n",
    "        documents=all_texts,\n",
    "        metadatas=all_metadatas,\n",
    "        embeddings=embeddings,\n",
    "        ids=all_ids\n",
    "    )\n",
    "    collection.add(\n",
    "        documents=all_texts,\n",
    "        metadatas=all_metadatas,\n",
    "        ids=all_ids\n",
    "    )\n",
    "\n",
    "    print(f\"  ‚úÖ –£—Ä–æ–≤–µ–Ω—å '{level}' —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ Chroma.\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ç–º–ø–ª—ã\n",
    "SAMPLES_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(SAMPLES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_sample_chunks, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"\\nüéâ –ò–Ω–≥–µ—Å—Ç –∑–∞–≤–µ—Ä—à—ë–Ω. –°—ç–º–ø–ª—ã —á–∞–Ω–∫–æ–≤: {SAMPLES_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed342c-8f1d-4e62-88a0-5ae7f573de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_or_create_collection(\n",
    "    name=level,\n",
    "    embedding_function=None  # –æ—Ç–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ-—ç–º–±–µ–¥–¥–∏–Ω–≥\n",
    ")\n",
    "        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "embeddings = embedding_model.encode(all_texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "embeddings = embeddings.tolist()  # Chroma –æ–∂–∏–¥–∞–µ—Ç list[list[float]]\n",
    "\n",
    "collection.add(\n",
    "    documents=all_texts,\n",
    "    metadatas=all_metadatas,\n",
    "    embeddings=embeddings,\n",
    "    ids=all_ids\n",
    ")\n",
    "collection.add(\n",
    "    documents=all_texts,\n",
    "    metadatas=all_metadatas,\n",
    "    ids=all_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "040df020-2abf-4abb-b21f-d3bea86d21e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_sample_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mall_sample_chunks\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'all_sample_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "all_sample_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbbd76c7-ddd4-47f9-ba03-b484a0208621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå –ù–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–π –≤ –±–∞–∑–µ.\n",
      "üìö –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏:\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# inspect_chunks.py\n",
    "from pathlib import Path\n",
    "import chromadb\n",
    "\n",
    "# –ü—É—Ç—å –∫ –≤–∞—à–µ–π Chroma-–±–∞–∑–µ\n",
    "VECTOR_DB_PATH = Path(\"./vectorstore\")  # –∏–ª–∏ \"../vectorstore\", –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏—è\n",
    "\n",
    "\n",
    "client = chromadb.PersistentClient(path=str(VECTOR_DB_PATH))\n",
    "collections = client.list_collections()\n",
    "\n",
    "if not collections:\n",
    "    print(\"‚ùå –ù–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–π –≤ –±–∞–∑–µ.\")\n",
    "    # return\n",
    "\n",
    "print(\"üìö –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏:\")\n",
    "for i, col in enumerate(collections):\n",
    "    print(f\"  {i+1}. {col.name} (—á–∞–Ω–∫–æ–≤: {col.count()})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# –ü—Ä–æ—Å–º–æ—Ç—Ä –≤—Å–µ—Ö –∫–æ–ª–ª–µ–∫—Ü–∏–π\n",
    "for collection in collections:\n",
    "    print(f\"\\nüîç –ö–æ–ª–ª–µ–∫—Ü–∏—è: {collection.name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 —á–∞–Ω–∫–∞\n",
    "    try:\n",
    "        data = collection.peek(limit=3)  # peek ‚Äî –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏: {e}\")\n",
    "        continue\n",
    "\n",
    "    n = len(data['ids'])\n",
    "    for i in range(n):\n",
    "        print(f\"\\n[ID] {data['ids'][i]}\")\n",
    "        print(f\"[–¢–µ–∫—Å—Ç] {data['documents'][i][:300]}{'...' if len(data['documents'][i]) > 300 else ''}\")\n",
    "        print(f\"[–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ] {data['metadatas'][i]}\")\n",
    "    \n",
    "    if n == 0:\n",
    "        print(\"  (–ø—É—Å—Ç–æ)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "796d2320-62c3-4bf6-a94a-f0c2b3097233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c73ab89e-8962-4423-af93-3bd2ddd7d38f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Collection [high_school] does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# –ü—Ä–∏–º–µ—Ä: –Ω–∞–π—Ç–∏ —á–∞–Ω–∫–∏ –∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m collection = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhigh_school\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m results = collection.get(\n\u001b[32m      4\u001b[39m     where={\u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mhigh_school/10-11 –∫–ª–∞—Å—Å—ã/–ê–ª–≥–µ–±—Ä–∞ –∏ –Ω–∞—á–∞–ª–∞ –º–∞—Ç. –∞–Ω–∞–ª–∏–∑–∞. 11–∫–ª. –ß.2. –ó–∞–¥–∞—á–Ω–∏–∫.doc\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m      5\u001b[39m )\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m–ù–∞–π–¥–µ–Ω–æ —á–∞–Ω–∫–æ–≤: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results[\u001b[33m'\u001b[39m\u001b[33mids\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/last-docling/lib/python3.12/site-packages/chromadb/api/client.py:208\u001b[39m, in \u001b[36mClient.get_collection\u001b[39m\u001b[34m(self, name, embedding_function, data_loader)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_collection\u001b[39m(\n\u001b[32m    201\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    206\u001b[39m     data_loader: Optional[DataLoader[Loadable]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    207\u001b[39m ) -> Collection:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_server\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m     persisted_ef_config = model.configuration_json.get(\u001b[33m\"\u001b[39m\u001b[33membedding_function\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    215\u001b[39m     validate_embedding_function_conflict_on_get(\n\u001b[32m    216\u001b[39m         embedding_function, persisted_ef_config\n\u001b[32m    217\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/last-docling/lib/python3.12/site-packages/chromadb/api/rust.py:270\u001b[39m, in \u001b[36mRustBindingsAPI.get_collection\u001b[39m\u001b[34m(self, name, tenant, database)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_collection\u001b[39m(\n\u001b[32m    265\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    268\u001b[39m     database: \u001b[38;5;28mstr\u001b[39m = DEFAULT_DATABASE,\n\u001b[32m    269\u001b[39m ) -> CollectionModel:\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     collection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CollectionModel(\n\u001b[32m    272\u001b[39m         \u001b[38;5;28mid\u001b[39m=collection.id,\n\u001b[32m    273\u001b[39m         name=collection.name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    279\u001b[39m         database=collection.database,\n\u001b[32m    280\u001b[39m     )\n",
      "\u001b[31mNotFoundError\u001b[39m: Collection [high_school] does not exist"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä: –Ω–∞–π—Ç–∏ —á–∞–Ω–∫–∏ –∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\n",
    "collection = client.get_collection(\"high_school\")\n",
    "results = collection.get(\n",
    "    where={\"source\": \"high_school/10-11 –∫–ª–∞—Å—Å—ã/–ê–ª–≥–µ–±—Ä–∞ –∏ –Ω–∞—á–∞–ª–∞ –º–∞—Ç. –∞–Ω–∞–ª–∏–∑–∞. 11–∫–ª. –ß.2. –ó–∞–¥–∞—á–Ω–∏–∫.doc\"}\n",
    ")\n",
    "print(f\"–ù–∞–π–¥–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(results['ids'])}\")\n",
    "for i in range(min(2, len(results['ids']))):\n",
    "    print(\"‚Äî\", results['documents'][i][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd962c34-eaaf-471d-9bb9-13e18f90ea3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33d3274-a166-42b4-abd0-3e91754a97c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b26ba-4ee1-4c9f-822b-1d8b37c5f237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4953f3c-2bd5-4ea8-9904-5a27c95f6200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9efb8d2-54a9-41cc-a39d-c1f1dffeca95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bdaa15-7d21-4eb0-9854-bd4542d6ea05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ff3d8-603d-45a7-b4a5-0aa72879c47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd993b-8644-4b13-acb4-c3b3da5c1503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64386da6-0985-408e-b2ce-14f05a9768de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb83fe-b487-4366-9237-70b92ae5aead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d68fcbf-9cd6-4037-85bb-cf00a7b23166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è –£—Ä–æ–≤–µ–Ω—å 'elementary' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç (–ø–∞–ø–∫–∞ ../data/elementary –Ω–µ –Ω–∞–π–¥–µ–Ω–∞). –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.\n",
      "\n",
      "üìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞ —É—Ä–æ–≤–Ω—è: middle_school\n",
      "  –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ (middle_school):  13%|‚ñà‚ñè       | 5/39 [01:50<12:30, 22.07s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     documents = \u001b[43mload_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m documents:\n\u001b[32m     41\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mload_document\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m     85\u001b[39m     tmp_pdf_path = Path(tmp_pdf.name)\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º DJVU ‚Üí PDF\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m result = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mddjvu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-format=pdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtmp_pdf_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m120\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# –º–∞–∫—Å 2 –º–∏–Ω—É—Ç—ã –Ω–∞ —Ñ–∞–π–ª\u001b[39;49;00m\n\u001b[32m     93\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.returncode != \u001b[32m0\u001b[39m:\n\u001b[32m     96\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ DJVU ‚Üí PDF: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.stderr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/last-docling/lib/python3.12/subprocess.py:550\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Popen(*popenargs, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m         stdout, stderr = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    552\u001b[39m         process.kill()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/last-docling/lib/python3.12/subprocess.py:1209\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1206\u001b[39m     endtime = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1208\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1209\u001b[39m     stdout, stderr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1211\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1212\u001b[39m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[32m   1213\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/last-docling/lib/python3.12/subprocess.py:2108\u001b[39m, in \u001b[36mPopen._communicate\u001b[39m\u001b[34m(self, input, endtime, orig_timeout)\u001b[39m\n\u001b[32m   2101\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_timeout(endtime, orig_timeout,\n\u001b[32m   2102\u001b[39m                         stdout, stderr,\n\u001b[32m   2103\u001b[39m                         skip_check_and_raise=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   2104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[32m   2105\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   2106\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mfailed to raise TimeoutExpired.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2108\u001b[39m ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2109\u001b[39m \u001b[38;5;28mself\u001b[39m._check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[32m   2111\u001b[39m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[32m   2112\u001b[39m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/last-docling/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "all_metadata = []\n",
    "for level in LEVELS:\n",
    "    level_dir = DATA_DIR / level\n",
    "    if not level_dir.exists():\n",
    "        print(f\"‚ö†Ô∏è –£—Ä–æ–≤–µ–Ω—å '{level}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç (–ø–∞–ø–∫–∞ {level_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞). –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nüìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞ —É—Ä–æ–≤–Ω—è: {level}\")\n",
    "    file_paths = get_all_document_files(level_dir)\n",
    "    print(f\"  –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(file_paths)}\")\n",
    "\n",
    "    if not file_paths:\n",
    "        print(f\"  ‚ö†Ô∏è –ù–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ {level_dir}\")\n",
    "        continue\n",
    "\n",
    "    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —á–∞–Ω–∫–∏–Ω–≥–∞\n",
    "    params = CHUNK_PARAMS.get(level, CHUNK_PARAMS[\"university\"])\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=params[\"chunk_size\"],\n",
    "        chunk_overlap=params[\"chunk_overlap\"],\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "    all_metadatas = []\n",
    "    all_texts = []\n",
    "    all_ids = []\n",
    "    metadata = {}\n",
    "\n",
    "    for file_path in tqdm(file_paths, desc=f\"  –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ ({level})\"):\n",
    "        if 'checkpoint' in file_path.name:\n",
    "            continue\n",
    "        try:\n",
    "            documents = load_document(file_path)\n",
    "            if not documents:\n",
    "                continue\n",
    "\n",
    "            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏\n",
    "            # chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "            grade = extract_grade_from_path(file_path, level_dir)\n",
    "            # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–±—É–¥–µ—Ç –ø–æ–∫–∞–∑–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é)\n",
    "            source_rel_path = str(file_path.relative_to(DATA_DIR))\n",
    "\n",
    "            # for i, chunk in enumerate(chunks):\n",
    "            #     text = chunk.page_content.strip()\n",
    "            #     if not text:\n",
    "            #         continue\n",
    "\n",
    "            #     metadata = {\n",
    "            #         \"level\": level,\n",
    "            #         \"grade\": grade,\n",
    "            #         \"source\": source_rel_path,  # <-- —ç—Ç–æ –±—É–¥–µ—Ç –≤ –æ—Ç–≤–µ—Ç–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è!\n",
    "            #         \"filename\": file_path.name,\n",
    "            #     }\n",
    "            #     # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ PDF)\n",
    "            #     if hasattr(chunk, 'metadata') and isinstance(chunk.metadata, dict):\n",
    "            #         metadata.update({\n",
    "            #             k: v for k, v in chunk.metadata.items()\n",
    "            #             if isinstance(v, (str, int, float, bool)) and k not in metadata\n",
    "            #         })\n",
    "\n",
    "            #     chunk_id = f\"{level}_{file_path.stem}_{i}\"\n",
    "\n",
    "            #     all_texts.append(text)\n",
    "            #     all_metadatas.append(metadata)\n",
    "            #     all_ids.append(chunk_id)\n",
    "\n",
    "            #     # –°–æ–±–∏—Ä–∞–µ–º —Å—ç–º–ø–ª—ã (–Ω–µ –±–æ–ª–µ–µ 10 —á–∞–Ω–∫–æ–≤ –≤—Å–µ–≥–æ)\n",
    "            #     if len(all_sample_chunks) < 10:\n",
    "            #         all_sample_chunks.append({\n",
    "            #             \"id\": chunk_id,\n",
    "            #             \"text\": text[:200] + \"...\" if len(text) > 200 else text,\n",
    "            #             \"metadata\": metadata\n",
    "            #         })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        all_metadata.append(metadata)\n",
    "    \n",
    "    print(f\"  –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è —É—Ä–æ–≤–Ω—è '{level}': {len(all_texts)}\")\n",
    "\n",
    "    if not all_texts:\n",
    "        print(f\"  ‚ö†Ô∏è –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ {level}\")\n",
    "        continue\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Chroma\n",
    "    # collection = client.get_or_create_collection(\n",
    "    #     name=level,\n",
    "    #     embedding_function=embedding_fn\n",
    "    # )\n",
    "    # collection.add(\n",
    "    #     documents=all_texts,\n",
    "    #     metadatas=all_metadatas,\n",
    "    #     ids=all_ids\n",
    "    # )\n",
    "    print(f\"  ‚úÖ –£—Ä–æ–≤–µ–Ω—å '{level}' —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ Chroma.\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ç–º–ø–ª—ã\n",
    "# SAMPLES_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "# with open(SAMPLES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(all_sample_chunks, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"\\nüéâ –ò–Ω–≥–µ—Å—Ç –∑–∞–≤–µ—Ä—à—ë–Ω. –°—ç–º–ø–ª—ã —á–∞–Ω–∫–æ–≤: {SAMPLES_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42214f56-ef08-46e6-9e82-153a831c0c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{}, {}, {}, {}, {}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7855932e-ae63-4add-a434-1d786f67fca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "last-docking",
   "language": "python",
   "name": "last-docling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
